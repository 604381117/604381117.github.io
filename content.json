{"meta":{"title":"荒野之萍","subtitle":"忙着活或忙着死","description":"后台 / 系统","author":"杨宇","url":"https://icoty.github.io","root":"/"},"pages":[{"title":"","date":"2019-05-02T10:46:47.000Z","updated":"2019-05-02T10:46:46.000Z","comments":true,"path":"nachos-3.4-MIPS.html","permalink":"https://icoty.github.io/nachos-3.4-MIPS.html","excerpt":"","text":"https://www.cnblogs.com/thoupin/p/4018455.html 当执行一条指令时，首先需要根据PC中存放的指令地址，将指令由内存取到指令寄存器中，此过程称为“取指令”。与此同时，PC中的地址或自动加1或由转移指针给出下一条指令的地址。此后经过分析指令，执行指令。完成第一条指令的执行，而后根据PC取出第二条指令的地址，如此循环，执行每一条指令。"},{"title":"分类","date":"2019-04-12T22:45:46.000Z","updated":"2019-04-13T09:28:17.000Z","comments":true,"path":"categories/index.html","permalink":"https://icoty.github.io/categories/index.html","excerpt":"","text":""},{"title":"关于","date":"2019-04-12T22:41:48.000Z","updated":"2019-04-27T08:23:30.000Z","comments":true,"path":"about/index.html","permalink":"https://icoty.github.io/about/index.html","excerpt":"","text":"ME QQ：604381117 博客：https://icoty.github.io CSDN：https://blog.csdn.net/qq_36347375 博客园：https://www.cnblogs.com/icoty23 Leetcode：https://github.com/icoty/LeetCode SKILLS Linux/vim/Makefile/gcc/gdb Mysql C/C++/STL TCP/Socket/Epoll/IPC Shell Docker Http Nginx 数据结构 微信小程序开发 Django/MVP框架 ThinkPHP5/MVC框架"},{"title":"杨宇","date":"2019-04-12T22:41:48.000Z","updated":"2019-04-26T08:19:48.000Z","comments":true,"path":"about/resume.html","permalink":"https://icoty.github.io/about/resume.html","excerpt":"","text":"ME 男 / 1992 QQ/WeChat：604381117 1864030** / icoty.yangy@gmail.com 技术博客：https://icoty.github.io Leetcode：https://github.com/icoty/LeetCode EDUCATION 北京大学 - 软件工程 / 2018.09～至今 东北大学 - 工业工程 / 2011.09～2015.07 EXPERIENCE浙江大华（ 2018年2月 ~ 2018年9月 ）第三方SDK集成 项目说明：客户的平台通过调用中间SDK API与公司内部平台通信。 功能实现： 在SDK进程中嵌入一个RTSP Client从公司内部平台拉取音视频流之后调用客户的SDK API接口直接推给客户的平台； 在SDK进程中开启一个RPC Client与公司的内部平台通信，SDK进程接收到SDK API的通知消息（由客户的平台发送过来）后立即封装成RPC请求交给RPC Client处理。 相关技术：RTSP拉流，RPC，网络编程，函数指针与函数回调。 第三方智能算法集成 项目说明：内部应用采集的音视频流，需要进行智能算法分析（如人脸检测，区域检测，车辆检测等），客户自己提供了一套算法库来分析这些过程，同时客户也自己开发了一个web界面需要与其自己的算法库通信，由于网络安全限制，客户的web请求不能直接与其自己的算法库通信，必须经由内部应用转发。 功能实现： 三方算法库进程集成HttpServer专职接受第三方web请求，然后回调至算法库内部； 三方算法库进程集成RpcClient专职接受经算法库处理后的元数据，然后发送到内部应用的RpcServer端口进行处理； 开辟两个共享内存专职把内部采集的到音视频传送到算法库进程。 相关技术：Http，共享内存，SDK。 矩阵元（深圳）技术 （ 2017年4月 ~ 2017年10月 ）Jenkins持续集成工具 项目说明：通过Jenkins管理产品分支，持续集成工具部署 功能实现： Jenkins搭建与配置； Redhat与Centos平台的版本功能持续验证，Docker平台的探索； Shell脚本，集代码分支、编译、测试、安装、Docker镜像制作和版本发布为一体； 对接Android、IOS、Web端联调、输出详细设计文档。 ARTICLE 进程间通信-利用共享内存和管道通信实现聊天窗口 SKILLS Linux/vim/Makefile/gcc/gdb Mysql C/C++/STL TCP/Socket/Epoll/IPC Shell Docker Http Nginx 数据结构 微信小程序开发 Django/MVP框架 ThinkPHP5/MVC框架"},{"title":"标签","date":"2019-04-12T22:45:34.000Z","updated":"2019-04-13T09:28:12.000Z","comments":true,"path":"tags/index.html","permalink":"https://icoty.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"epoll源码分析(基于linux-5.1.4)","slug":"epoll-source","date":"2019-06-03T12:49:38.000Z","updated":"2019-06-08T02:33:33.000Z","comments":true,"path":"2019/06/03/epoll-source/","link":"","permalink":"https://icoty.github.io/2019/06/03/epoll-source/","excerpt":"","text":"APIepoll提供给用户进程的接口有如下四个，本文基于linux-5.1.4源码详细分析每个API具体做了啥工作，通过UML时序图理清内核内部的函数调用关系。 int epoll_create1(int size)； 创建一个epfd句柄，size为0时等价于int epoll_create(0)。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)； 向epfd上添加/修改/删除fd。 int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout)； 返回所有就绪的fd。 内核数据结构先上一张UML类图，从整体进行把握，图中已经标出各个数据结构所在的文件。 下面贴出各个数据结构代码，切记，实际在过代码的时候，其实我们没有必要对每一个变量和每一行代码咬文嚼字，也不建议这样去做，我们只需要重点关注主要的数据成员和那些关键的代码行，把心思和精力投入到我们最该关注的那部分，从框架层面去把握整体，抓准各个模块的核心，各个模块之间如何耦合，如何同步，如何通信等，这才是能够让你快速进步的最优路线。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145/** Each file descriptor added to the eventpoll interface will* have an entry of this type linked to the \"rbr\" RB tree.* Avoid increasing the size of this struct, there can be many thousands* of these on a server and we do not want this to take another cache line.*/struct epitem &#123; union &#123; /* RB tree node links this structure to the eventpoll RB tree */ struct rb_node rbn; /* Used to free the struct epitem */ struct rcu_head rcu; &#125;; /* List header used to link this structure to the eventpoll ready list */ struct list_head rdllink; /* * Works together \"struct eventpoll\"-&gt;ovflist in keeping the * single linked chain of items. */ struct epitem *next; /* The file descriptor information this item refers to */ struct epoll_filefd ffd; /* Number of active wait queue attached to poll operations */ int nwait; /* List containing poll wait queues */ struct list_head pwqlist; /* The \"container\" of this item */ struct eventpoll *ep; /* List header used to link this item to the \"struct file\" items list */ struct list_head fllink; /* wakeup_source used when EPOLLWAKEUP is set */ struct wakeup_source __rcu *ws; /* The structure that describe the interested events and the source fd */ struct epoll_event event;&#125;;/** This structure is stored inside the \"private_data\" member of the file* structure and represents the main data structure for the eventpoll* interface.*/struct eventpoll &#123; /* * This mutex is used to ensure that files are not removed * while epoll is using them. This is held during the event * collection loop, the file cleanup path, the epoll file exit * code and the ctl operations. */ struct mutex mtx; /* Wait queue used by sys_epoll_wait() */ wait_queue_head_t wq; /* Wait queue used by file-&gt;poll() */ wait_queue_head_t poll_wait; /* List of ready file descriptors */ struct list_head rdllist; /* Lock which protects rdllist and ovflist */ rwlock_t lock; /* RB tree root used to store monitored fd structs */ struct rb_root_cached rbr; /* * This is a single linked list that chains all the \"struct epitem\" that * happened while transferring ready events to userspace w/out * holding -&gt;lock. */ struct epitem *ovflist; /* wakeup_source used when ep_scan_ready_list is running */ struct wakeup_source *ws; /* The user that created the eventpoll descriptor */ struct user_struct *user; struct file *file; /* used to optimize loop detection check */ int visited; struct list_head visited_list_link;#ifdef CONFIG_NET_RX_BUSY_POLL /* used to track busy poll napi_id */ unsigned int napi_id;#endif&#125;;/* eppoll_entry主要完成epitem和epitem事件发生时的callback（ep_poll_callback） * 函数之间的关联，并将上述两个数据结构包装成一个链表节点， * 挂载到目标文件file的waithead中。 * Wait structure used by the poll hooks */struct eppoll_entry &#123; /* List header used to link this structure to the \"struct epitem\" */ struct list_head llink; /* The \"base\" pointer is set to the container \"struct epitem\" */ struct epitem *base; /* * Wait queue item that will be linked to the target file wait * queue head. */ wait_queue_entry_t wait; /* The wait queue head that linked the \"wait\" wait queue item */ wait_queue_head_t *whead;&#125;;/* ep_pqueue主要完成epitem和callback函数的关联。 * 然后通过目标文件的poll函数调用callback函数ep_ptable_queue_proc。 * Poll函数一般由设备驱动提供，以网络设备为例， * 他的poll函数为sock_poll然后根据sock类型调用不同的poll函数如： * packet_poll。packet_poll在通过datagram_poll调用sock_poll_wait， * 最后在poll_wait实际调用callback函数（ep_ptable_queue_proc） * Wrapper struct used by poll queueing */struct ep_pqueue &#123; poll_table pt; struct epitem *epi;&#125;;/* Used by the ep_send_events() function as callback private data */struct ep_send_events_data &#123; int maxevents; struct epoll_event __user *events; int res;&#125;;struct fd &#123; struct file *file; unsigned int flags;&#125;; 全局调用关系再贴一张各个API从用户进程陷入到内核态并执行系统调用的详细过程，以及client发数据过来时触发ep_poll_callback回调函数的执行流程。 epoll模块初始化&amp;内存池开辟epoll是内核的一个module，内核启动时会初始化这个module。 123456789101112131415161718192021222324252627282930313233343536373839404142434445// fs/eventpoll.cstatic int __init eventpoll_init(void)&#123; struct sysinfo si; si_meminfo(&amp;si); /* * Allows top 4% of lomem to be allocated for epoll watches (per user). */ max_user_watches = (((si.totalram - si.totalhigh) / 25) &lt;&lt; PAGE_SHIFT) / EP_ITEM_COST; BUG_ON(max_user_watches &lt; 0); /* * Initialize the structure used to perform epoll file descriptor * inclusion loops checks. */ ep_nested_calls_init(&amp;poll_loop_ncalls);#ifdef CONFIG_DEBUG_LOCK_ALLOC /* Initialize the structure used to perform safe poll wait head wake ups */ ep_nested_calls_init(&amp;poll_safewake_ncalls);#endif /* * We can have many thousands of epitems, so prevent this from * using an extra cache line on 64-bit (and smaller) CPUs */ BUILD_BUG_ON(sizeof(void *) &lt;= 8 &amp;&amp; sizeof(struct epitem) &gt; 128); // 提前开辟eventpoll_epi内存池,UML时序图的第21步alloc时直接从内存池里取, // 而不是重新调用malloc,效率得以提高 /* Allocates slab cache used to allocate \"struct epitem\" items */ epi_cache = kmem_cache_create(\"eventpoll_epi\", sizeof(struct epitem),0, placehold_flag, 0); // 提前开辟eventpoll_pwq内存池,UML时序图的第28步alloc时直接从内存池里取 // 而不是重新调用malloc,效率得以提高 /* Allocates slab cache used to allocate \"struct eppoll_entry\" */ pwq_cache = kmem_cache_create(\"eventpoll_pwq\", sizeof(struct eppoll_entry), 0, SLAB_PANIC|SLAB_ACCOUNT, NULL); return 0;&#125;fs_initcall(eventpoll_init); epoll_create用户空间调用epoll_create(0)或epoll_create1(int)，其实质就是在名为”eventpollfs”的文件系统里创建了一个新文件，同时为该文件申请一个fd，绑定一个inode，最后返回该文件句柄。 epoll_create/epoll_create1陷入内核123456789101112// fs/eventpoll.cSYSCALL_DEFINE1(epoll_create1, int, flags)&#123; return do_epoll_create(flags);&#125;SYSCALL_DEFINE1(epoll_create, int, size)&#123; if (size &lt;= 0) return -EINVAL; return do_epoll_create(0);&#125; do_epoll_create/ep_alloc1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/* * fs/eventpoll.c * Open an eventpoll file descriptor. */static int do_epoll_create(int flags)&#123; int error, fd; struct eventpoll *ep = NULL; struct file *file; /* Check the EPOLL_* constant for consistency. */ BUILD_BUG_ON(EPOLL_CLOEXEC != O_CLOEXEC); if (flags &amp; ~EPOLL_CLOEXEC) return -EINVAL; /* * 申请一个struct eventpoll内存空间,执行初始化后赋给ep * Create the internal data structure (\"struct eventpoll\"). */ error = ep_alloc(&amp;ep); if (error &lt; 0) return error; /* * 获取一个未使用的fd句柄 * Creates all the items needed to setup an eventpoll file. That is, * a file structure and a free file descriptor. */ fd = get_unused_fd_flags(O_RDWR | (flags &amp; O_CLOEXEC)); if (fd &lt; 0) &#123; error = fd; goto out_free_ep; &#125; file = anon_inode_getfile(\"[eventpoll]\", &amp;eventpoll_fops, ep, O_RDWR | (flags &amp; O_CLOEXEC)); if (IS_ERR(file)) &#123; error = PTR_ERR(file); goto out_free_fd; &#125; ep-&gt;file = file; // 绑定fd和file fd_install(fd, file); // 这个fd就是epfd句柄,返回给用户进程的 return fd;out_free_fd: put_unused_fd(fd);out_free_ep: ep_free(ep); return error;&#125;// fs/eventpoll.c// 形参是一个二级指针,该接口就是简单的分配一个struct eventpoll,然后执行初始化工作static int ep_alloc(struct eventpoll **pep)&#123; int error; struct user_struct *user; struct eventpoll *ep; user = get_current_user(); error = -ENOMEM; ep = kzalloc(sizeof(*ep), GFP_KERNEL); if (unlikely(!ep)) goto free_uid; mutex_init(&amp;ep-&gt;mtx); rwlock_init(&amp;ep-&gt;lock); init_waitqueue_head(&amp;ep-&gt;wq); init_waitqueue_head(&amp;ep-&gt;poll_wait); INIT_LIST_HEAD(&amp;ep-&gt;rdllist); ep-&gt;rbr = RB_ROOT_CACHED; ep-&gt;ovflist = EP_UNACTIVE_PTR; ep-&gt;user = user; *pep = ep; return 0;free_uid: free_uid(user); return error;&#125; anon_inode_getfile/alloc_file_pseudo/alloc_file123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121/** * fs/anon_inodes.c * anon_inode_getfile - creates a new file instance by hooking it up to an * anonymous inode, and a dentry that describe the \"class\" * of the file * * @name: [in] name of the \"class\" of the new file * @fops: [in] file operations for the new file * @priv: [in] private data for the new file (will be file's private_data) * @flags: [in] flags * * Creates a new file by hooking it on a single inode. This is useful for files * that do not need to have a full-fledged inode in order to operate correctly. * All the files created with anon_inode_getfile() will share a single inode, * hence saving memory and avoiding code duplication for the file/inode/dentry * setup. Returns the newly created file* or an error pointer. * 在一个inode上挂接一个新文件,这对于不需要完整inode才能正确操作的文件非常有用。 * 使用anon_inode_getfile()创建的所有文件都将共享一个inode， * 因此可以节省内存并避免文件/inode/dentry设置的代码重复。 * 返回新创建的文件*或错误指针。 */struct file *anon_inode_getfile(const char *name,const struct file_operations *fops,void *priv, int flags)&#123; struct file *file; if (IS_ERR(anon_inode_inode)) return ERR_PTR(-ENODEV); if (fops-&gt;owner &amp;&amp; !try_module_get(fops-&gt;owner)) return ERR_PTR(-ENOENT); /* * We know the anon_inode inode count is always greater than zero, * so ihold() is safe. */ ihold(anon_inode_inode); // 创建一个名字为“[eventpoll]”的eventpollfs文件描述符 file = alloc_file_pseudo(anon_inode_inode, anon_inode_mnt, name, flags &amp; (O_ACCMODE | O_NONBLOCK), fops); if (IS_ERR(file)) goto err; file-&gt;f_mapping = anon_inode_inode-&gt;i_mapping; // file-&gt;private_data指向传进来的priv( = struct eventpoll *ep) file-&gt;private_data = priv; return file;err: iput(anon_inode_inode); module_put(fops-&gt;owner); return file;&#125;EXPORT_SYMBOL_GPL(anon_inode_getfile);// fs/file_table.cstruct file *alloc_file_pseudo(struct inode *inode, struct vfsmount *mnt, const char *name, int flags,const struct file_operations *fops)&#123; static const struct dentry_operations anon_ops = &#123; .d_dname = simple_dname &#125;; struct qstr this = QSTR_INIT(name, strlen(name)); struct path path; struct file *file; // 挂载名为“[eventpoll]”的eventpollfs文件系统 path.dentry = d_alloc_pseudo(mnt-&gt;mnt_sb, &amp;this); if (!path.dentry) return ERR_PTR(-ENOMEM); if (!mnt-&gt;mnt_sb-&gt;s_d_op) d_set_d_op(path.dentry, &amp;anon_ops); path.mnt = mntget(mnt); d_instantiate(path.dentry, inode); // inode和file绑定，返回绑定后的file结构 file = alloc_file(&amp;path, flags, fops); if (IS_ERR(file)) &#123; ihold(inode); path_put(&amp;path); &#125; return file;&#125;EXPORT_SYMBOL(alloc_file_pseudo);/** * fs/file_table.c * alloc_file - allocate and initialize a 'struct file' * * @path: the (dentry, vfsmount) pair for the new file * @flags: O_... flags with which the new file will be opened * @fop: the 'struct file_operations' for the new file */static struct file *alloc_file(const struct path *path, int flags, const struct file_operations *fop)&#123; struct file *file; // 申请一个空的file结构 file = alloc_empty_file(flags, current_cred()); if (IS_ERR(file)) return file; file-&gt;f_path = *path; file-&gt;f_inode = path-&gt;dentry-&gt;d_inode; file-&gt;f_mapping = path-&gt;dentry-&gt;d_inode-&gt;i_mapping; file-&gt;f_wb_err = filemap_sample_wb_err(file-&gt;f_mapping); if ((file-&gt;f_mode &amp; FMODE_READ) &amp;&amp; likely(fop-&gt;read || fop-&gt;read_iter)) file-&gt;f_mode |= FMODE_CAN_READ; if ((file-&gt;f_mode &amp; FMODE_WRITE) &amp;&amp; likely(fop-&gt;write || fop-&gt;write_iter)) file-&gt;f_mode |= FMODE_CAN_WRITE; file-&gt;f_mode |= FMODE_OPENED; file-&gt;f_op = fop; if ((file-&gt;f_mode &amp; (FMODE_READ | FMODE_WRITE)) == FMODE_READ) i_readcount_inc(path-&gt;dentry-&gt;d_inode); return file;&#125; epoll_ctl用户进程调用int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)，op可填EPOLL_CTL_ADD(注册fd到epfd)、EPOLL_CTL_MOD(修改已注册fd监听的事件)和EPOLL_CTL_DEL(从epfd中删除fd)。 epoll_ctl陷入内核123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171/* * fs/eventpoll.c * The following function implements the controller interface for * the eventpoll file that enables the insertion/removal/change of * file descriptors inside the interest set. */SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event)&#123; int error; int full_check = 0; struct fd f, tf; struct eventpoll *ep; struct epitem *epi; struct epoll_event epds; struct eventpoll *tep = NULL; error = -EFAULT; // copy_from_user将用户空间关注的event事件拷贝到内核空间 if (ep_op_has_event(op) &amp;&amp; copy_from_user(&amp;epds, event, sizeof(struct epoll_event))) goto error_return; error = -EBADF; f = fdget(epfd); if (!f.file) goto error_return; /* Get the \"struct file *\" for the target file */ tf = fdget(fd); if (!tf.file) goto error_fput; /* The target file descriptor must support poll */ error = -EPERM; if (!file_can_poll(tf.file)) goto error_tgt_fput; /* 如果系统设置了自动休眠模式（通过/sys/power/autosleep）， * 当唤醒设备的事件发生时，设备驱动会保持唤醒状态，直到事件进入排队状态。 * 为了保持设备唤醒直到事件处理完成，必须使用epoll EPOLLWAKEUP 标记。 * 一旦给structe poll_event中的events字段设置了EPOLLWAKEUP标记，系统会在事件排队时就保持唤醒， * 从epoll_wait调用开始，持续要下一次epoll_wait调用。 */ /* Check if EPOLLWAKEUP is allowed */ if (ep_op_has_event(op)) ep_take_care_of_epollwakeup(&amp;epds); /* * We have to check that the file structure underneath the file descriptor * the user passed to us _is_ an eventpoll file. And also we do not permit * adding an epoll file descriptor inside itself. */ error = -EINVAL; if (f.file == tf.file || !is_file_epoll(f.file)) goto error_tgt_fput; /* * epoll adds to the wakeup queue at EPOLL_CTL_ADD time only, * so EPOLLEXCLUSIVE is not allowed for a EPOLL_CTL_MOD operation. * Also, we do not currently supported nested exclusive wakeups. */ if (ep_op_has_event(op) &amp;&amp; (epds.events &amp; EPOLLEXCLUSIVE)) &#123; if (op == EPOLL_CTL_MOD) goto error_tgt_fput; if (op == EPOLL_CTL_ADD &amp;&amp; (is_file_epoll(tf.file) || (epds.events &amp; ~EPOLLEXCLUSIVE_OK_BITS))) goto error_tgt_fput; &#125; /* * At this point it is safe to assume that the \"private_data\" contains * our own data structure. */ ep = f.file-&gt;private_data; /* * When we insert an epoll file descriptor, inside another epoll file * descriptor, there is the change of creating closed loops, which are * better be handled here, than in more critical paths. While we are * checking for loops we also determine the list of files reachable * and hang them on the tfile_check_list, so we can check that we * haven't created too many possible wakeup paths. * * We do not need to take the global 'epumutex' on EPOLL_CTL_ADD when * the epoll file descriptor is attaching directly to a wakeup source, * unless the epoll file descriptor is nested. The purpose of taking the * 'epmutex' on add is to prevent complex toplogies such as loops and * deep wakeup paths from forming in parallel through multiple * EPOLL_CTL_ADD operations. */ mutex_lock_nested(&amp;ep-&gt;mtx, 0); if (op == EPOLL_CTL_ADD) &#123; if (!list_empty(&amp;f.file-&gt;f_ep_links) || is_file_epoll(tf.file)) &#123; full_check = 1; mutex_unlock(&amp;ep-&gt;mtx); mutex_lock(&amp;epmutex); if (is_file_epoll(tf.file)) &#123; error = -ELOOP; if (ep_loop_check(ep, tf.file) != 0) &#123; clear_tfile_check_list(); goto error_tgt_fput; &#125; &#125; else list_add(&amp;tf.file-&gt;f_tfile_llink, &amp;tfile_check_list); mutex_lock_nested(&amp;ep-&gt;mtx, 0); if (is_file_epoll(tf.file)) &#123; tep = tf.file-&gt;private_data; mutex_lock_nested(&amp;tep-&gt;mtx, 1); &#125; &#125; &#125; /* * Try to lookup the file inside our RB tree, Since we grabbed \"mtx\" * above, we can be sure to be able to use the item looked up by * ep_find() till we release the mutex. * 从红黑树中寻找添加的fd是否存在，存在则返回到ep中，否则返回NULL */ epi = ep_find(ep, tf.file, fd); error = -EINVAL; switch (op) &#123; case EPOLL_CTL_ADD: // 若ep为空说明红黑树中不存在,执行ep_insert添加到红黑树中 if (!epi) &#123; epds.events |= EPOLLERR | EPOLLHUP; // 如果不存在则添加，已经存在不重复添加 error = ep_insert(ep, &amp;epds, tf.file, fd, full_check); &#125; else error = -EEXIST; if (full_check) clear_tfile_check_list(); break; // 删除fd调用ep_remove case EPOLL_CTL_DEL: if (epi) error = ep_remove(ep, epi); else error = -ENOENT; break; // 修改已注册fd所监听的事件,调用ep_modify case EPOLL_CTL_MOD: if (epi) &#123; if (!(epi-&gt;event.events &amp; EPOLLEXCLUSIVE)) &#123; epds.events |= EPOLLERR | EPOLLHUP; error = ep_modify(ep, epi, &amp;epds); &#125; &#125; else error = -ENOENT; break; &#125; if (tep != NULL) mutex_unlock(&amp;tep-&gt;mtx); mutex_unlock(&amp;ep-&gt;mtx);error_tgt_fput: if (full_check) mutex_unlock(&amp;epmutex); fdput(tf);error_fput: fdput(f);error_return: return error;&#125; ep_find12345678910111213141516171819202122232425262728293031/* * fs/eventpoll.c * Search the file inside the eventpoll tree. The RB tree operations * are protected by the \"mtx\" mutex, and ep_find() must be called with * \"mtx\" held. */static struct epitem *ep_find(struct eventpoll *ep, struct file *file, int fd)&#123; int kcmp; struct rb_node *rbp; struct epitem *epi, *epir = NULL; struct epoll_filefd ffd; ep_set_ffd(&amp;ffd, file, fd); // 从红黑树根节开始二分查找,判断左右子树 for (rbp = ep-&gt;rbr.rb_root.rb_node; rbp; ) &#123; epi = rb_entry(rbp, struct epitem, rbn); kcmp = ep_cmp_ffd(&amp;ffd, &amp;epi-&gt;ffd); if (kcmp &gt; 0) rbp = rbp-&gt;rb_right; else if (kcmp &lt; 0) rbp = rbp-&gt;rb_left; else &#123; epir = epi; break; &#125; &#125; return epir;&#125; ep_insert123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151/* * fs/eventpoll.c * Must be called with \"mtx\" held. */static int ep_insert(struct eventpoll *ep, const struct epoll_event *event, struct file *tfile, int fd, int full_check)&#123; int error, pwake = 0; __poll_t revents; long user_watches; struct epitem *epi; struct ep_pqueue epq; lockdep_assert_irqs_enabled(); user_watches = atomic_long_read(&amp;ep-&gt;user-&gt;epoll_watches); if (unlikely(user_watches &gt;= max_user_watches)) return -ENOSPC; // epi_cache内存池在epoll模块初始化时已经分配,这里根据slab直接取一个epitem if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL))) return -ENOMEM; // 初始化 epitem /* Item initialization follow here ... */ INIT_LIST_HEAD(&amp;epi-&gt;rdllink); INIT_LIST_HEAD(&amp;epi-&gt;fllink); INIT_LIST_HEAD(&amp;epi-&gt;pwqlist); epi-&gt;ep = ep; ep_set_ffd(&amp;epi-&gt;ffd, tfile, fd); epi-&gt;event = *event; epi-&gt;nwait = 0; epi-&gt;next = EP_UNACTIVE_PTR; if (epi-&gt;event.events &amp; EPOLLWAKEUP) &#123; error = ep_create_wakeup_source(epi); if (error) goto error_create_wakeup_source; &#125; else &#123; RCU_INIT_POINTER(epi-&gt;ws, NULL); &#125; // 创建一个struct ep_pqueue epq, 并与epitem(epi)关联 /* Initialize the poll table using the queue callback */ epq.epi = epi; /* 设置epq的回调函数为ep_ptable_queue_proc,当调用poll_wait时会调用该回调函数， * 而函数体ep_ptable_queue_proc内部所做的主要工作, * 就是把epitem对应fd的事件到来时的回调函数设置为ep_poll_callback。 * ep_poll_callback所做的主要工作就是把就绪的fd放到就绪链表rdllist上, * 然后唤醒epoll_wait的调用者, 被唤醒的进程再把rdllist上就绪的fd的events拷贝给用户进程, * 完成一个闭环。 */ init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc); /* * Attach the item to the poll hooks and get current event bits. * We can safely use the file* here because its usage count has * been increased by the caller of this function. Note that after * this operation completes, the poll callback can start hitting * the new item. * 判断当前插入的event是否刚好发生，返回就绪事件的掩码赋给revents, * 如果发生，那么做一个ready动作， * 后面的if语句将epitem加入到rdlist中，并对epoll上的wait队列调用wakeup */ revents = ep_item_poll(epi, &amp;epq.pt, 1); /* * We have to check if something went wrong during the poll wait queue * install process. Namely an allocation for a wait queue failed due * high memory pressure. */ error = -ENOMEM; if (epi-&gt;nwait &lt; 0) goto error_unregister; /* Add the current item to the list of active epoll hook for this file */ spin_lock(&amp;tfile-&gt;f_lock); // 每个文件会将所有监听自己的epitem链起来 list_add_tail_rcu(&amp;epi-&gt;fllink, &amp;tfile-&gt;f_ep_links); spin_unlock(&amp;tfile-&gt;f_lock); /* * Add the current item to the RB tree. All RB tree operations are * protected by \"mtx\", and ep_insert() is called with \"mtx\" held. * 将epitem插入到对应的eventpoll红黑树中去,红黑树用一个互斥锁进行保护 */ ep_rbtree_insert(ep, epi); /* now check if we've created too many backpaths */ error = -EINVAL; if (full_check &amp;&amp; reverse_path_check()) goto error_remove_epi; /* We have to drop the new item inside our item list to keep track of it */ write_lock_irq(&amp;ep-&gt;lock); /* record NAPI ID of new item if present */ ep_set_busy_poll_napi_id(epi); /* If the file is already \"ready\" we drop it inside the ready list */ if (revents &amp;&amp; !ep_is_linked(epi)) &#123; list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); /* Notify waiting tasks that events are available */ if (waitqueue_active(&amp;ep-&gt;wq)) wake_up(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; &#125; write_unlock_irq(&amp;ep-&gt;lock); atomic_long_inc(&amp;ep-&gt;user-&gt;epoll_watches); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;ep-&gt;poll_wait); return 0;error_remove_epi: spin_lock(&amp;tfile-&gt;f_lock); list_del_rcu(&amp;epi-&gt;fllink); spin_unlock(&amp;tfile-&gt;f_lock); rb_erase_cached(&amp;epi-&gt;rbn, &amp;ep-&gt;rbr);error_unregister: ep_unregister_pollwait(ep, epi); /* * We need to do this because an event could have been arrived on some * allocated wait queue. Note that we don't care about the ep-&gt;ovflist * list, since that is used/cleaned only inside a section bound by \"mtx\". * And ep_insert() is called with \"mtx\" held. */ write_lock_irq(&amp;ep-&gt;lock); if (ep_is_linked(epi)) list_del_init(&amp;epi-&gt;rdllink); write_unlock_irq(&amp;ep-&gt;lock); wakeup_source_unregister(ep_wakeup_source(epi));error_create_wakeup_source: kmem_cache_free(epi_cache, epi); return error;&#125; kmem_cache_alloc1234567891011121314151617181920212223/** * slab算法从内存池cachep中分配一个实例返回 * mm/slab.c * kmem_cache_alloc - Allocate an object * @cachep: The cache to allocate from. * @flags: See kmalloc(). * * Allocate an object from this cache. The flags are only relevant * if the cache has no available objects. * * Return: pointer to the new object or %NULL in case of error */void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)&#123; void *ret = slab_alloc(cachep, flags, _RET_IP_); trace_kmem_cache_alloc(_RET_IP_, ret, cachep-&gt;object_size, cachep-&gt;size, flags); return ret;&#125;EXPORT_SYMBOL(kmem_cache_alloc); init_poll_funcptr/ep_ptable_queue_proc/ep_poll_callback/init_waitqueue_func_entryinit_poll_funcptr：设置epq的回调函数为ep_ptable_queue_proc，当调用poll_wait时会调用该回调函数；ep_ptable_queue_proc：该函数内部所做的主要工作，就是把epitem对应fd的事件到来时的回调函数设置为ep_poll_callback。ep_poll_callback：主要工作就是把就绪的fd放到就绪链表rdllist上，然后唤醒epoll_wait的调用者，被唤醒的进程再把rdllist上就绪的fd的events拷贝给用户进程，完成一个闭环。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183/* * 设置回调 * include/linux/poll.h */static inline void init_poll_funcptr(poll_table *pt, poll_queue_proc qproc)&#123; pt-&gt;_qproc = qproc; pt-&gt;_key = ~(__poll_t)0; /* all events enabled */&#125;/* * This is the callback that is used to add our wait queue to the * target file wakeup lists. * struct file *file（目标文件）= epi-&gt;ffd.file, * wait_queue_head_t *whead（目标文件的waitlist）= eventpoll-&gt;poll_wait, * poll_table *pt（前面生成的poll_table） */static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,poll_table *pt)&#123; struct epitem *epi = ep_item_from_epqueue(pt); // 创建一个struct eppoll_entry,与对应的epitem关联上 struct eppoll_entry *pwq; // 从pwq_cache内存池中取一个struct eppoll_entry if (epi-&gt;nwait &gt;= 0 &amp;&amp; (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) &#123; // 把每个epitem对应的回调函数设置为ep_poll_callback, // 当epitem关注的事件中断到来时会执行回调函数ep_poll_callback init_waitqueue_func_entry(&amp;pwq-&gt;wait, ep_poll_callback); pwq-&gt;whead = whead; // 关联上epitem pwq-&gt;base = epi; // 通过add_wait_queue将epoll_entry挂载到目标文件的waitlist。 // 完成这个动作后，epoll_entry已经被挂载到waitlist if (epi-&gt;event.events &amp; EPOLLEXCLUSIVE) add_wait_queue_exclusive(whead, &amp;pwq-&gt;wait); else add_wait_queue(whead, &amp;pwq-&gt;wait); // eppoll_entry-&gt;llink执行epitem-&gt;pwqlist list_add_tail(&amp;pwq-&gt;llink, &amp;epi-&gt;pwqlist); epi-&gt;nwait++; &#125; else &#123; /* We have to signal that an error occurred */ epi-&gt;nwait = -1; &#125;&#125;// include/linux/wait.hstatic inline void init_waitqueue_func_entry(struct wait_queue_entry *wq_entry, wait_queue_func_t func)&#123; wq_entry-&gt;flags = 0; wq_entry-&gt;private = NULL; wq_entry-&gt;func = func;&#125;/* * fs/eventpoll.c * This is the callback that is passed to the wait queue wakeup * mechanism. It is called by the stored file descriptors when they * have events to report. * * This callback takes a read lock in order not to content with concurrent * events from another file descriptors, thus all modifications to -&gt;rdllist * or -&gt;ovflist are lockless. Read lock is paired with the write lock from * ep_scan_ready_list(), which stops all list modifications and guarantees * that lists state is seen correctly. * * Another thing worth to mention is that ep_poll_callback() can be called * concurrently for the same @epi from different CPUs if poll table was inited * with several wait queues entries. Plural wakeup from different CPUs of a * single wait queue is serialized by wq.lock, but the case when multiple wait * queues are used should be detected accordingly. This is detected using * cmpxchg() operation. */static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)&#123; int pwake = 0; struct epitem *epi = ep_item_from_wait(wait); struct eventpoll *ep = epi-&gt;ep; __poll_t pollflags = key_to_poll(key); unsigned long flags; int ewake = 0; read_lock_irqsave(&amp;ep-&gt;lock, flags); ep_set_busy_poll_napi_id(epi); /* * If the event mask does not contain any poll(2) event, we consider the * descriptor to be disabled. This condition is likely the effect of the * EPOLLONESHOT bit that disables the descriptor when an event is received, * until the next EPOLL_CTL_MOD will be issued. */ if (!(epi-&gt;event.events &amp; ~EP_PRIVATE_BITS)) goto out_unlock; /* * Check the events coming with the callback. At this stage, not * every device reports the events in the \"key\" parameter of the * callback. We need to be able to handle both cases here, hence the * test for \"key\" != NULL before the event match test. */ if (pollflags &amp;&amp; !(pollflags &amp; epi-&gt;event.events)) goto out_unlock; /* * If we are transferring events to userspace, we can hold no locks * (because we're accessing user memory, and because of linux f_op-&gt;poll() * semantics). All the events that happen during that period of time are * chained in ep-&gt;ovflist and requeued later on. */ if (READ_ONCE(ep-&gt;ovflist) != EP_UNACTIVE_PTR) &#123; // epi-&gt;next == EP_UNACTIVE_PTR说明rdllist当前被其他进程持有, // 因此调用chain_epi_lockless把epitem放入vovflist上 if (epi-&gt;next == EP_UNACTIVE_PTR &amp;&amp; chain_epi_lockless(epi)) ep_pm_stay_awake_rcu(epi); goto out_unlock; &#125; // rdllist抢占成功,调用list_add_tail_lockless把epitem挂入rdllist上 /* If this file is already in the ready list we exit soon */ if (!ep_is_linked(epi) &amp;&amp; list_add_tail_lockless(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist)) &#123; ep_pm_stay_awake_rcu(epi); &#125; /* * Wake up ( if active ) both the eventpoll wait list and the -&gt;poll() * wait list. */ if (waitqueue_active(&amp;ep-&gt;wq)) &#123; if ((epi-&gt;event.events &amp; EPOLLEXCLUSIVE) &amp;&amp; !(pollflags &amp; POLLFREE)) &#123; switch (pollflags &amp; EPOLLINOUT_BITS) &#123; case EPOLLIN: if (epi-&gt;event.events &amp; EPOLLIN) ewake = 1; break; case EPOLLOUT: if (epi-&gt;event.events &amp; EPOLLOUT) ewake = 1; break; case 0: ewake = 1; break; &#125; &#125; // 同时唤醒eventpoll的wq等待队列,也就是唤醒poll_wait的调用者 wake_up(&amp;ep-&gt;wq); &#125; if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; out_unlock: read_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;ep-&gt;poll_wait); if (!(epi-&gt;event.events &amp; EPOLLEXCLUSIVE)) ewake = 1; if (pollflags &amp; POLLFREE) &#123; /* * If we race with ep_remove_wait_queue() it can miss * -&gt;whead = NULL and do another remove_wait_queue() after * us, so we can't use __remove_wait_queue(). */ list_del_init(&amp;wait-&gt;entry); /* * -&gt;whead != NULL protects us from the race with ep_free() * or ep_remove(), ep_remove_wait_queue() takes whead-&gt;lock * held by the caller. Once we nullify it, nothing protects * ep/epi or even wait. */ smp_store_release(&amp;ep_pwq_from_wait(wait)-&gt;whead, NULL); &#125; return ewake;&#125; ep_item_poll/poll_wait/ep_scan_ready_list123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152/* * Differs from ep_eventpoll_poll() in that internal callers already have * the ep-&gt;mtx so we need to start from depth=1, such that mutex_lock_nested() * is correctly annotated. */static __poll_t ep_item_poll(const struct epitem *epi, poll_table *pt,int depth)&#123; struct eventpoll *ep; bool locked; pt-&gt;_key = epi-&gt;event.events; if (!is_file_epoll(epi-&gt;ffd.file)) return vfs_poll(epi-&gt;ffd.file, pt) &amp; epi-&gt;event.events; // 拿到eventpoll,回头过去看UML数据结构,private_data是指向eventpoll的 ep = epi-&gt;ffd.file-&gt;private_data; // 这里面会执行前面设置的ep_ptable_queue_proc回调体 // ep_ptable_queue_proc函数体的工作在前面已经介绍过 poll_wait(epi-&gt;ffd.file, &amp;ep-&gt;poll_wait, pt); locked = pt &amp;&amp; (pt-&gt;_qproc == ep_ptable_queue_proc); // 把就绪链表rdllist拷贝到用户空间 return ep_scan_ready_list(epi-&gt;ffd.file-&gt;private_data, ep_read_events_proc, &amp;depth, depth, locked) &amp; epi-&gt;event.events;&#125;// include/linux/poll.hstatic inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)&#123; if (p &amp;&amp; p-&gt;_qproc &amp;&amp; wait_address) p-&gt;_qproc(filp, wait_address, p);&#125;/** * ep_scan_ready_list - Scans the ready list in a way that makes possible for * the scan code, to call f_op-&gt;poll(). Also allows for * O(NumReady) performance. * * @ep: Pointer to the epoll private data structure. * @sproc: Pointer to the scan callback. * @priv: Private opaque data passed to the @sproc callback. * @depth: The current depth of recursive f_op-&gt;poll calls. * @ep_locked: caller already holds ep-&gt;mtx * * Returns: The same integer error code returned by the @sproc callback. */static __poll_t ep_scan_ready_list(struct eventpoll *ep, __poll_t (*sproc)(struct eventpoll *, struct list_head *, void *),void *priv, int depth, bool ep_locked)&#123; __poll_t res; int pwake = 0; struct epitem *epi, *nepi; LIST_HEAD(txlist); lockdep_assert_irqs_enabled(); /* * We need to lock this because we could be hit by * eventpoll_release_file() and epoll_ctl(). */ if (!ep_locked) mutex_lock_nested(&amp;ep-&gt;mtx, depth); /* * Steal the ready list, and re-init the original one to the * empty list. Also, set ep-&gt;ovflist to NULL so that events * happening while looping w/out locks, are not lost. We cannot * have the poll callback to queue directly on ep-&gt;rdllist, * because we want the \"sproc\" callback to be able to do it * in a lockless way. */ write_lock_irq(&amp;ep-&gt;lock); // 把就绪链表rdllist赋给临时的txlist,执行该操作后rdllist会被清空, // 因为rdllist需要腾出来给其他进程继续往上放内容, // 从而把txlist内epitem对应fd的就绪events复制到用户空间 list_splice_init(&amp;ep-&gt;rdllist, &amp;txlist); WRITE_ONCE(ep-&gt;ovflist, NULL); write_unlock_irq(&amp;ep-&gt;lock); /* * sproc就是前面设置好的ep_poll_callback,事件到来了执行该回调体, * sproc会把就绪的epitem放入rdllist或ovflist上 * Now call the callback function. */ res = (*sproc)(ep, &amp;txlist, priv); write_lock_irq(&amp;ep-&gt;lock); /* * During the time we spent inside the \"sproc\" callback, some * other events might have been queued by the poll callback. * We re-insert them inside the main ready-list here. */ for (nepi = READ_ONCE(ep-&gt;ovflist); (epi = nepi) != NULL; nepi = epi-&gt;next, epi-&gt;next = EP_UNACTIVE_PTR) &#123; /* * We need to check if the item is already in the list. * During the \"sproc\" callback execution time, items are * queued into -&gt;ovflist but the \"txlist\" might already * contain them, and the list_splice() below takes care of them. */ if (!ep_is_linked(epi)) &#123; /* * -&gt;ovflist is LIFO, so we have to reverse it in order * to keep in FIFO. */ list_add(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); &#125; &#125; /* * We need to set back ep-&gt;ovflist to EP_UNACTIVE_PTR, so that after * releasing the lock, events will be queued in the normal way inside * ep-&gt;rdllist. */ WRITE_ONCE(ep-&gt;ovflist, EP_UNACTIVE_PTR); /* * 把水平触发EPOLLLT属性的epitem依旧挂回到rdllist, * 因为我们希望即使没有新的数据到来,只要数据还没被用户空间读完,就继续上报 * Quickly re-inject items left on \"txlist\". */ list_splice(&amp;txlist, &amp;ep-&gt;rdllist); __pm_relax(ep-&gt;ws); if (!list_empty(&amp;ep-&gt;rdllist)) &#123; /* * Wake up (if active) both the eventpoll wait list and * the -&gt;poll() wait list (delayed after we release the lock). * wake_up唤醒epoll_wait的调用者 */ if (waitqueue_active(&amp;ep-&gt;wq)) wake_up(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; &#125; write_unlock_irq(&amp;ep-&gt;lock); if (!ep_locked) mutex_unlock(&amp;ep-&gt;mtx); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;ep-&gt;poll_wait); return res;&#125; 到此，epoll_ctl的分析就已经完了，这里只描述的EPOLL_CTL_ADD调用。EPOLL_CTL_MOD/EPOLL_CTL_DEL相对就简单很多，这三个操作差异主要体现在fs/eventpoll.c文件内接口SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd,struct epoll_event __user*, event)的switch语句部分，EPOLL_CTL_MOD和EPOLL_CTL_DEL分别对应ep_modify和ep_remove，这两个函数就是从红黑树中去找到对应的节点进行修改和删除操作，因此这里没有贴代码。 epoll_waitepoll_wait陷入内核123456// fs/eventpoll.cSYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events,int, maxevents, int, timeout)&#123; return do_epoll_wait(epfd, events, maxevents, timeout);&#125; do_epoll_wait/ep_poll/ep_send_events/ep_send_events_proc123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296/* * Implement the event wait interface for the eventpoll file. It is the kernel * part of the user space epoll_wait(2). */static int do_epoll_wait(int epfd, struct epoll_event __user *events, int maxevents, int timeout)&#123; int error; // struct fd结构在数据结构部分代码已经列出 struct fd f; struct eventpoll *ep; /* The maximum number of event must be greater than zero */ if (maxevents &lt;= 0 || maxevents &gt; EP_MAX_EVENTS) return -EINVAL; /* Verify that the area passed by the user is writeable */ if (!access_ok(events, maxevents * sizeof(struct epoll_event))) return -EFAULT; /* Get the \"struct file *\" for the eventpoll file */ f = fdget(epfd); if (!f.file) return -EBADF; /* * We have to check that the file structure underneath the fd * the user passed to us _is_ an eventpoll file. */ error = -EINVAL; if (!is_file_epoll(f.file)) goto error_fput; /* * At this point it is safe to assume that the \"private_data\" contains * our own data structure. * 直接拿到eventpoll对象 */ ep = f.file-&gt;private_data; // ep_poll时主循环体,当rdllist为空时调用者根据设置的超时参数, // 决定是等待还是返回 /* Time to fish for events ... */ error = ep_poll(ep, events, maxevents, timeout);error_fput: fdput(f); return error;&#125;/** * ep_poll - Retrieves ready events, and delivers them to the caller supplied * event buffer. * * @ep: Pointer to the eventpoll context. * @events: Pointer to the userspace buffer where the ready events should be * stored. * @maxevents: Size (in terms of number of events) of the caller event buffer. * @timeout: Maximum timeout for the ready events fetch operation, in * milliseconds. If the @timeout is zero, the function will not block, * while if the @timeout is less than zero, the function will block * until at least one event has been retrieved (or an error * occurred). * * Returns: Returns the number of ready events which have been fetched, or an * error code, in case of error. */static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout)&#123; int res = 0, eavail, timed_out = 0; u64 slack = 0; bool waiter = false; wait_queue_entry_t wait; ktime_t expires, *to = NULL; lockdep_assert_irqs_enabled(); // 超时设置 if (timeout &gt; 0) &#123; struct timespec64 end_time = ep_set_mstimeout(timeout); slack = select_estimate_accuracy(&amp;end_time); to = &amp;expires; *to = timespec64_to_ktime(end_time); &#125; else if (timeout == 0) &#123; // 立即返回 /* * Avoid the unnecessary trip to the wait queue loop, if the * caller specified a non blocking operation. We still need * lock because we could race and not see an epi being added * to the ready list while in irq callback. Thus incorrectly * returning 0 back to userspace. */ timed_out = 1; write_lock_irq(&amp;ep-&gt;lock); eavail = ep_events_available(ep); write_unlock_irq(&amp;ep-&gt;lock); goto send_events; &#125;// 否则是永久等待,直到有新的事件到来fetch_events: if (!ep_events_available(ep)) ep_busy_loop(ep, timed_out); eavail = ep_events_available(ep); if (eavail) goto send_events; /* * Busy poll timed out. Drop NAPI ID for now, we can add * it back in when we have moved a socket with a valid NAPI * ID onto the ready list. */ ep_reset_busy_poll_napi_id(ep); /* * We don't have any available event to return to the caller. We need * to sleep here, and we will be woken by ep_poll_callback() when events * become available. */ if (!waiter) &#123; waiter = true; // ep-&gt;rdllist存放的是已就绪(read)的fd，为空时说明当前没有就绪的fd, // 创建一个等待队列,并使用当前进程（current）初始化 init_waitqueue_entry(&amp;wait, current); spin_lock_irq(&amp;ep-&gt;wq.lock); // 将当前进程添加到等待队列 __add_wait_queue_exclusive(&amp;ep-&gt;wq, &amp;wait); spin_unlock_irq(&amp;ep-&gt;wq.lock); &#125; for (;;) &#123; /* * We don't want to sleep if the ep_poll_callback() sends us * a wakeup in between. That's why we set the task state * to TASK_INTERRUPTIBLE before doing the checks. */ set_current_state(TASK_INTERRUPTIBLE); /* * Always short-circuit for fatal signals to allow * threads to make a timely exit without the chance of * finding more events available and fetching * repeatedly. */ if (fatal_signal_pending(current)) &#123; res = -EINTR; break; &#125; // ep_events_available内部会判断rdllist是否为空 eavail = ep_events_available(ep); if (eavail) break; // 循环体,如果rdllist不为空,则跳出循环体,进入send_events if (signal_pending(current)) &#123; res = -EINTR; break; &#125; if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) &#123; timed_out = 1; break; &#125; &#125; __set_current_state(TASK_RUNNING);send_events: /* * Try to transfer events to user space. In case we get 0 events and * there's still timeout left over, we go trying again in search of * more luck. * ep_send_events接口复制txlist内epitem对应fd的就绪events到用户空间 */ if (!res &amp;&amp; eavail &amp;&amp; !(res = ep_send_events(ep, events, maxevents)) &amp;&amp; !timed_out) goto fetch_events; if (waiter) &#123; spin_lock_irq(&amp;ep-&gt;wq.lock); // 将当前进程移出等待队列 __remove_wait_queue(&amp;ep-&gt;wq, &amp;wait); spin_unlock_irq(&amp;ep-&gt;wq.lock); &#125; return res;&#125;fs/eventpoll.cstatic int ep_send_events(struct eventpoll *ep, struct epoll_event __user *events, int maxevents)&#123; struct ep_send_events_data esed; esed.maxevents = maxevents; esed.events = events; // 传入ep_send_events_proc ep_scan_ready_list(ep, ep_send_events_proc, &amp;esed, 0, false); return esed.res;&#125;// 实际执行复制到用户空间的工作是由该函数体负责static __poll_t ep_send_events_proc(struct eventpoll *ep, struct list_head *head,void *priv)&#123; struct ep_send_events_data *esed = priv; __poll_t revents; struct epitem *epi, *tmp; struct epoll_event __user *uevent = esed-&gt;events; struct wakeup_source *ws; poll_table pt; init_poll_funcptr(&amp;pt, NULL); esed-&gt;res = 0; /* * We can loop without lock because we are passed a task private list. * Items cannot vanish during the loop because ep_scan_ready_list() is * holding \"mtx\" during this call. */ lockdep_assert_held(&amp;ep-&gt;mtx); // lambda表达式 list_for_each_entry_safe(epi, tmp, head, rdllink) &#123; if (esed-&gt;res &gt;= esed-&gt;maxevents) break; /* * Activate ep-&gt;ws before deactivating epi-&gt;ws to prevent * triggering auto-suspend here (in case we reactive epi-&gt;ws * below). * * This could be rearranged to delay the deactivation of epi-&gt;ws * instead, but then epi-&gt;ws would temporarily be out of sync * with ep_is_linked(). */ ws = ep_wakeup_source(epi); if (ws) &#123; if (ws-&gt;active) __pm_stay_awake(ep-&gt;ws); __pm_relax(ws); &#125; list_del_init(&amp;epi-&gt;rdllink); /* * If the event mask intersect the caller-requested one, * deliver the event to userspace. Again, ep_scan_ready_list() * is holding ep-&gt;mtx, so no operations coming from userspace * can change the item. */ revents = ep_item_poll(epi, &amp;pt, 1); if (!revents) continue; // 复制到用户空间 if (__put_user(revents, &amp;uevent-&gt;events) || __put_user(epi-&gt;event.data, &amp;uevent-&gt;data)) &#123; list_add(&amp;epi-&gt;rdllink, head); ep_pm_stay_awake(epi); if (!esed-&gt;res) esed-&gt;res = -EFAULT; return 0; &#125; esed-&gt;res++; uevent++; if (epi-&gt;event.events &amp; EPOLLONESHOT) epi-&gt;event.events &amp;= EP_PRIVATE_BITS; else if (!(epi-&gt;event.events &amp; EPOLLET)) &#123; /* * If this file has been added with Level * Trigger mode, we need to insert back inside * the ready list, so that the next call to * epoll_wait() will check again the events * availability. At this point, no one can insert * into ep-&gt;rdllist besides us. The epoll_ctl() * callers are locked out by * ep_scan_ready_list() holding \"mtx\" and the * poll callback will queue them in ep-&gt;ovflist. */ list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); &#125; &#125; return 0;&#125; 参考文献[1] epoll react[2] linux epoll源码分析[3] IO复用select/poll/epoll[4] IO复用epoll[5] linux epoll源码[6] linux poll/epoll实现[7] linux源码github仓库","categories":[{"name":"IO多路复用模型","slug":"IO多路复用模型","permalink":"https://icoty.github.io/categories/IO多路复用模型/"},{"name":"UML","slug":"UML","permalink":"https://icoty.github.io/categories/UML/"},{"name":"linux源码","slug":"linux源码","permalink":"https://icoty.github.io/categories/linux源码/"}],"tags":[{"name":"epoll","slug":"epoll","permalink":"https://icoty.github.io/tags/epoll/"}]},{"title":"基于线程池、消息队列和epoll模型实现并发服务器架构","slug":"cs-threadpool-message-queue","date":"2019-05-25T08:16:55.000Z","updated":"2019-06-08T03:23:09.000Z","comments":true,"path":"2019/05/25/cs-threadpool-message-queue/","link":"","permalink":"https://icoty.github.io/2019/05/25/cs-threadpool-message-queue/","excerpt":"","text":"引言并发是什么？企业在进行产品开发过程中为什么需要考虑这个问题？想象一下天猫的双11和京东的618活动，一秒的点击量就有几十万甚至上百万，这么多请求一下子涌入到服务器，服务器需要对这么多的请求逐个进行消化掉，假如服务器一秒的处理能力就几万，那么剩下的不能及时得到处理的这些请求作何处理？总不能让用户界面一直等着，因此消息队列应运而生，所有的请求都统一放入消息队列，工作线程从消息队列不断的消费，消息队列相当于一个缓冲区，可达到解藕、异步和削峰的目的。 Kafka、ActiveMQ、RabbitMQ和RockerMQ都是消息队列的典型，每一种都有其自身的优势和劣势。本文我用自己编写的Buffer类模拟消息队列，如果是企业级需要上线的应用，一般都是基于业界已有的MQ框架上开发。 需求原型 N个Client从标准输入接收数据，然后连续不断的发送到Server端； Server端接收来自每个Client的数据，将数据中的小写字母全部转换成大写字母，其他字符保持不变，最后把转换结果发送给对应的Client。 需求分解 拿到需求，第一步要做的就是分析需求并选择合适的设计架构，考虑到Server需要和Client进行通信，Client来自四面八方，端对端通信自然选择TCP，因此Server端需要能够监听新的连接请求和已有连接的业务请求； 又由于Server需要响应多个Client的业务请求，我们希望把业务处理交给Server端的工作线程（消费者）来做； 同时还需要一个IO线程负责监听Socket描述符，当IO线程监听到已有连接的业务请求时，立即把请求内容封装成一个任务推入消息队列尾； IO线程与工作线程互斥访问消息队列，当然工作线程消费一个任务或者IO线程添加一个任务都需要通知对方，也就是同步； 工作线程处理完毕后，把处理结果交给IO线程，由IO线程负责把结果发送给对应的Client，也就是IO线程与工作线程的分离，这里工作线程通知IO线程的方式我用eventfd来实现； 我们希望引入Log4cpp记录服务端的日志，并能够保存到文件中； 分析完这些，一个整体架构和大体的样子在脑海中就已经形成了，接着就需要编写设计文档和画流程图、类图和时序图了。 详细设计文档 UML静态类图： UML动态时序图： 效果 如图，开了三个Client，运行结果正确： Server端通过Log4cpp把日志写到文件中： 源码获取https://github.com/icoty/cs_threadpool_epoll_mq 目录结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859.├── client // 客户端Demo│ ├── Client.cc│ ├── Client.exe│ ├── client.sh // 进入该目录下启动Client Demo: sh client.sh│ ├── Log4func.cc // 引入日志模块重新疯转│ ├── Log4func.h│ └── Makefile // 编译方式：make├── conf│ └── my.conf // IP,Port配置文件, 从这里进行修改├── include // 头文件│ ├── Configuration.hpp // 配置文件,单例类,my.conf的内存化│ ├── FileName.hpp // 全局定义,Configuration会用到│ ├── log // 日志模块头文件│ │ └── Log4func.hpp│ ├── net // 网络框架模块头文件│ │ ├── EpollPoller.hpp│ │ ├── InetAddress.hpp│ │ ├── Socket.hpp│ │ ├── SockIO.hpp│ │ ├── TcpConnection.hpp│ │ └── TcpServer.hpp│ ├── String2Upper.hpp // 工作线程转换成大写实际走的这里面的接口│ ├── String2UpperServer.hpp // Server端的整个工厂│ └── threadpool // 线程池、锁、条件变量和消息队列的封装│ ├── Buffer.hpp│ ├── Condition.hpp│ ├── MutexLock.hpp│ ├── Noncopyable.hpp│ ├── Pthread.hpp│ ├── Task.hpp│ └── Threadpool.hpp├── log // Server端的日志通过Log4cpp记录到这个文件中│ └── log4test.log├── Makefile // 编译方式：make├── README.md ├── server // server端Demo│ ├── server.exe│ └── server.sh // 进入该目录下启动Server Demo：sh server.sh└── src // 源文件 ├── Configuration.cpp ├── log │ └── Log4func.cpp ├── main.cpp ├── net │ ├── EpollPoller.cpp │ ├── InetAddress.cpp │ ├── Socket.cpp │ ├── SockIO.cpp │ ├── TcpConnection.cpp │ └── TcpServer.cpp ├── String2Upper.cpp ├── String2UpperServer.cpp └── threadpool ├── Buffer.cpp ├── Condition.cpp ├── MutexLock.cpp // MutexLockGuard封装 ├── Pthread.cpp └── Threadpool.cpp 参考文献[1] UNIX环境高级编程第3版[2] cpp reference[3] UML时序图[4] Log4cpp官网下载[5] Log4cpp安装","categories":[{"name":"IO多路复用模型","slug":"IO多路复用模型","permalink":"https://icoty.github.io/categories/IO多路复用模型/"},{"name":"同步机制","slug":"同步机制","permalink":"https://icoty.github.io/categories/同步机制/"},{"name":"并发服务器架构","slug":"并发服务器架构","permalink":"https://icoty.github.io/categories/并发服务器架构/"},{"name":"互斥机制","slug":"互斥机制","permalink":"https://icoty.github.io/categories/互斥机制/"},{"name":"TCP","slug":"TCP","permalink":"https://icoty.github.io/categories/TCP/"},{"name":"线程池","slug":"线程池","permalink":"https://icoty.github.io/categories/线程池/"},{"name":"UML","slug":"UML","permalink":"https://icoty.github.io/categories/UML/"},{"name":"C++","slug":"C","permalink":"https://icoty.github.io/categories/C/"}],"tags":[{"name":"线程池","slug":"线程池","permalink":"https://icoty.github.io/tags/线程池/"},{"name":"TCP/Socket编程","slug":"TCP-Socket编程","permalink":"https://icoty.github.io/tags/TCP-Socket编程/"},{"name":"消息队列","slug":"消息队列","permalink":"https://icoty.github.io/tags/消息队列/"},{"name":"条件变量实现","slug":"条件变量实现","permalink":"https://icoty.github.io/tags/条件变量实现/"},{"name":"锁实现","slug":"锁实现","permalink":"https://icoty.github.io/tags/锁实现/"},{"name":"面向对象编程","slug":"面向对象编程","permalink":"https://icoty.github.io/tags/面向对象编程/"},{"name":"单例设计模式","slug":"单例设计模式","permalink":"https://icoty.github.io/tags/单例设计模式/"},{"name":"epoll","slug":"epoll","permalink":"https://icoty.github.io/tags/epoll/"},{"name":"Log4cpp","slug":"Log4cpp","permalink":"https://icoty.github.io/tags/Log4cpp/"},{"name":"Makefile","slug":"Makefile","permalink":"https://icoty.github.io/tags/Makefile/"},{"name":"智能指针","slug":"智能指针","permalink":"https://icoty.github.io/tags/智能指针/"},{"name":"UML","slug":"UML","permalink":"https://icoty.github.io/tags/UML/"},{"name":"类图","slug":"类图","permalink":"https://icoty.github.io/tags/类图/"},{"name":"时序图","slug":"时序图","permalink":"https://icoty.github.io/tags/时序图/"},{"name":"C++","slug":"C","permalink":"https://icoty.github.io/tags/C/"}]},{"title":"Hexo引入Mermaid流程图和MathJax数学公式","slug":"markdown-mermaid-mathjax","date":"2019-05-23T00:30:10.000Z","updated":"2019-05-24T02:38:01.000Z","comments":true,"path":"2019/05/23/markdown-mermaid-mathjax/","link":"","permalink":"https://icoty.github.io/2019/05/23/markdown-mermaid-mathjax/","excerpt":"","text":"近来用Markdown写文章，越来越不喜欢插入图片了，一切能用语法解决的问题坚决不放图，原因有二： 如果把流程图和数学公式都以图片方式放到文章内，当部署到Github上后，访问博客时图片加载实在太慢，有时一篇文章需要画10来个流程图，那你就得截图10来多次，还得给这些图片想一个合适的名字，同时插入图片的时候还要注意图片的插入位置和顺序； 如果你要把文章发布到其他博客平台，如CSDN、博客园，在每一个平台上你都要插入10来多次图片，作为程序员，这种笨拙又耗时的方法，我实在不能忍。 于是愤而搜索，Mermaid语法可实现流程图功能，MathJax语法可实现数学公式和特殊符号的功能，只需要遵循其语法规则即可，这也不由得让我想起：“苏乞儿打完降龙十八掌前17掌之后幡然领悟出第18掌的奥妙时说的那句话：我实在是太聪明了！”。下面都以next主题为例，我的主题是https://github.com/theme-next/hexo-theme-next Mermaid 如果你用的主题和我的主题仓库是同一个，你只需修改blog/themes/next/_config.yml内mermaid模块enable为true，其他的啥也不用做。 12345678910$cd blog/ # 走到博客根目录$yarn add hexo-filter-mermaid-diagrams # 安装mermaid插件# Mermaid tagmermaid: enable: true # Available themes: default | dark | forest | neutral theme: forest cdn: //cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js #cdn: //cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js 如果你的不是next主题或者你的next主题是github上旧版本仓库，你首先需要查看themes/next/_config.yml内是否有mermaid模块，如果有，按照前面的方法1，执行完方法1后，如果不奏效，不要改回去，接着下面的内容继续配置。如果没有mermaid模块，仍然着接下面内容继续配置。 编辑博客根目录下的blog/_config.yml，在最后添加如下内容： 123456# mermaid chartmermaid: ## mermaid url https://github.com/knsv/mermaid enable: true # default true version: \"7.1.2\" # default v7.1.2 options: # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js #startOnload: true // default true 编辑blog/themes/next/layout/_partials/footer.swig，在最后添加如下内容： 12345678&#123;% if theme.mermaid.enable %&#125; &lt;script src='https://unpkg.com/mermaid@&#123;&#123; theme.mermaid.version &#125;&#125;/dist/mermaid.min.js'&gt;&lt;/script&gt; &lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123;&#123; JSON.stringify(theme.mermaid.options) &#125;&#125;); &#125; &lt;/script&gt;&#123;% endif %&#125; 如果你的主题下没有footer.swig文件，你需要在你的主题目录下搜索文件名为after-footer.ejs和after_footer.pug的文件，根据文件名的不同在其最后添加不同的内容，这点在github上的 hexo-filter-mermaid-diagrams 教程已经明确交代了。123456789101112131415161718# 若是after_footer.pug，在最后添加内容if theme.mermaid.enable == true script(type='text/javascript', id='maid-script' mermaidoptioins=theme.mermaid.options src='https://unpkg.com/mermaid@'+ theme.mermaid.version + '/dist/mermaid.min.js' + '?v=' + theme.version) script. if (window.mermaid) &#123; var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins')); mermaid.initialize(options); &#125;# 若是after-footer.ejs，在最后添加&lt;% if (theme.mermaid.enable) &#123; %&gt; &lt;script src='https://unpkg.com/mermaid@&lt;%= theme.mermaid.version %&gt;/dist/mermaid.min.js'&gt;&lt;/script&gt; &lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123;theme: 'forest'&#125;); &#125; &lt;/script&gt;&lt;% &#125; %&gt; 最后，赶紧部署到github上观看效果吧，如果不奏效的话，把blog/_config.yml中的external_link设置为false和设置为true都试下，这点在github教程上也已经交代了，因为我的next版本不涉及这个问题，请君多试。1!!!Notice: if you want to use 'Class diagram', please edit your '_config.yml' file, set external_link: false. - hexo bug. 前两步做完后，如果都不奏效，这里还有一招绝杀技，那就是打开blog/public目录下你写的文章的index.html。 搜索“mermaid”，所有的流程图都应该是括在一个标签类的，如果你的流程图没有class = “mermaid”，那就是第一步安装的hexo-filter-mermaid-diagrams插件没有解析成功，可能是hexo，node，yarn版本问题所致。 12345# 流程图解析为：&lt;pre class=\"mermaid\"&gt;流程图&lt;/pre&gt;&lt;pre class=\"mermaid\"&gt;graph LRA[Bob&lt;br&gt;输入明文P] --&gt;|P|B[\"Bob的私钥PRbob&lt;br&gt;加密算法(如RSA)&lt;br&gt;C=E(PRbob,P)\"];B --&gt;|传输数字签名C|C[\"Alice的公钥环&#123;PUbob,……&#125;&lt;br&gt;解密算法(如RSA)&lt;br&gt;P=D(PUbob,C)\"];C --&gt;|P|D[\"Alice&lt;br&gt;输出明文P\"];&lt;/pre&gt; 若流程图确实解析成功了，但是web仍然不显示流程图，说明js文件引入失败，继续在index.html中搜索“mermaid.min.js”，正常情况下需要有如下内容，如果没有，在文件最后的”body”之前添加上，之后再部署观看效果，到此理论上应该可以了，如果还是不行，仔细检查下有没有遗漏步骤，考验你解bug的时候到了。 123456&lt;script src=\"https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js\"&gt;&lt;/script&gt;&lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123;theme: 'forest'&#125;); &#125;&lt;/script&gt; MathJax我的主题只需修改blog/themes/next/_config.yml内math模块enable为true即可，不需要安装任何插件，修改之后，在文章的Front Matter栏添加”mathjax: true”才能解析，其他主题也可以试下该方法可行否，都大同小异。12345678910111213141516171819202122232425262728293031# Math Equations Render Supportmath: enable: true # 这里改为true # Default (true) will load mathjax / katex script on demand. # That is it only render those page which has `mathjax: true` in Front Matter. # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE. per_page: true engine: mathjax #engine: katex # hexo-rendering-pandoc (or hexo-renderer-kramed) needed to full MathJax support. mathjax: cdn: //cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML #cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML # See: https://mhchem.github.io/MathJax-mhchem/ #mhchem: //cdn.jsdelivr.net/npm/mathjax-mhchem@3 #mhchem: //cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0 # hexo-renderer-markdown-it-plus (or hexo-renderer-markdown-it with markdown-it-katex plugin) needed to full Katex support. katex: cdn: //cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css #cdn: //cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css copy_tex: # See: https://github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex enable: false copy_tex_js: //cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.js copy_tex_css: //cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.css 12345678910# 文章引入方式---title: 常用加密算法的应用date: 2019-05-20 13:58:36tags: [对称加密算法,非对称加密算法/公钥算法,Hash函数/散列函数/摘要函数,消息认证,流密码,数字签名/指纹/消息摘要]categories: - [密码学与信息安全]copyright: truemathjax: true # 添加这行，文章才会解析--- 参考文献MathJax语法规则Mermaid语法规则Mermaid官方教程Mermaid Github仓库MathJax Github仓库","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://icoty.github.io/categories/Hexo/"}],"tags":[{"name":"Mermaid","slug":"Mermaid","permalink":"https://icoty.github.io/tags/Mermaid/"},{"name":"MathJax","slug":"MathJax","permalink":"https://icoty.github.io/tags/MathJax/"}]},{"title":"常用加密算法的应用","slug":"crypt","date":"2019-05-20T13:58:36.000Z","updated":"2019-05-23T06:55:41.000Z","comments":true,"path":"2019/05/20/crypt/","link":"","permalink":"https://icoty.github.io/2019/05/20/crypt/","excerpt":"","text":"实际工作和开发过程中，网络通信过程中的数据传输和存储大多需要经过严格的加解密设计，比如用户的登陆与注册，敏感信息传输，支付网站和银行的交易信息，甚至为了防止被拖库，数据库的敏感信息存储也需要经过精心的设计。在进行安全设计过程中，或多或少涉及到密码学的一些概念，比如对称加密算法，非对称加密算法(也名公钥算法)，消息认证，Hash函数(也名散列函数或摘要算法)，数字签名(也名指纹或摘要)，流密码等。 一直以来，对于这些概念，你是否有一种模棱两可，似懂非懂的感觉？下面咱们一起揭开密码学这层神秘的面纱。 基本概念密码体制密码体制是满足以下5个条件的五元组(P, C, K, E, D)，满足条件: P(Plaintext)是可能明文的有限集(明文空间); C(Ciphertext)是可能密文的有限集(密文空间); K(Key)是一切可能密钥构成的有限集(密钥空间); E(Encrtption)和D(Decryption)是分别由密钥决定的所有加密算法和解密算法的集合; 存在： $k \\in K$ ,有加密算法 $e_k:P \\rightarrow C$ , $e_k \\in E$；同时有由 $ {k_1} \\in K$ 决定的解密算法 $d_{k_1} : C \\rightarrow P，d_{k_1} \\in D$；满足关系 $d_{k_1} (e_k(x)) = x, x \\in P$。 密码破译密码破译根本目的在于破译出密钥或密文，假设破译者Oscar是在已知密码体制的前提下来破译Bob使用的密钥。这个假设被称为Kerckhoff准则，最常见的破解类型有如下5种，从1～5，Oscar的破译难度逐渐降低。 唯密文攻击：Oscar仅具有密文串c，Oscar只能通过统计特性分析密文串p的规律； 已知明文攻击：Oscar具有一些明文串p和相应的密文c，{p，c}可以是{P，C}的任意非空子集； 选择明文攻击：Oscar可获得对加密机的暂时访问，因此他能选择特定明文串p并构造出相应的密文串c； 选择密文攻击：Oscar可暂时接近解密机，因此他能选择特定密文串c并构造出相应的明文串p。 选择文本攻击：Oscar可以制造任意明文(p) / 密文(c)并得到对应的密文(c) / 明文(p)。 加密算法对称加密算法对称加密算法的加密密钥和解密密钥相同，常见的对称加密算法有：AES、DES、2DES、3DES、RC4、RC5、RC6，Blowfish和IDEA，目前使用最广泛的是DES、AES。 graph LR A[发送方Bob输入明文P] -->|P|B[\"发送方Bob与接收方Alice共用相同密钥K加密算法(如DES)C=E(K,P)\"]; B -->|传输密文C|C[\"接收方Alice与发送方Bob共用相同密钥K解密算法(如DES)P=D(K,C)\"]; C -->|P|D[\"Alice接收方输出明文P\"]; 非对称加密算法(公钥算法)公钥算法的加密算法和解密算法使用不同的密钥，分别为公钥和私钥，这两个密钥中的任何一个都可以用来加密，而另一个用来解密。常见的公钥算法有：椭圆曲线(ECC)、RSA、Diffie-Hellman、El Gamal(安全性建立在基于求解离散对数是困难的)、DSA(适用于数字签名)。 公钥算法的应用 发送方Bob用接收方Alice的公钥对消息进行加密，接收方Alice用自己的私钥进行解密，可提供消息传输过程中的保密性。 graph LR A[Bob输入明文P] -->|P|B[\"Bob的公钥环{PUalice,……}加密算法C=E(PUalice,P)\"]; B -->|传输密文C|C[\"Alice私钥PRalice解密算法P=D(PRalice,C)\"]; C -->|P|D[\"Alice输出明文P\"]; 发送方Bob采用自己的私钥对明文进行加密，虽然任何持有Bob公钥的人都能够解密，但是只有拥有Bob私钥的人才能产生密文C，而Bob的私钥只有自己知道，因此密文C也叫做数字签名，数字签名C可用于认证源和数据的完整性。 graph LR A[Bob输入明文P] -->|P|B[\"Bob的私钥PRbob加密算法(如RSA)C=E(PRbob,P)\"]; B -->|传输数字签名C|C[\"Alice的公钥环{PUbob,……}解密算法(如RSA)P=D(PUbob,C)\"]; C -->|P|D[\"Alice输出明文P\"]; 发送方Bob首先采用自己的私钥对明文进行加密，然后使用接收方Alice的公钥再进行一次加密后传输，则既可提供认证功能，又可提供消息传输过程中的保密性。 graph LR A[Bob输入明文P] -->|P|B[\"Bob的私钥PRbob加密算法(如RSA)C=E(PRbob,P)\"]; B -->|数字签名C|C[\"Bob的公钥环{PUalice,……}加密算法(如RSA)C1=E(PUalice,C)\"]; C -->|传输密文C1|D[\"Alice的私钥PRalice解密算法(如RSA)C=D(PRalice,C1)\"]; D -->|数字签名C|E[\"Alice的公钥环{PUbob,……}解密算法(如RSA)P=D(PUbob,C)\"]; E -->|P|F[\"Alice输出明文P\"]; 发送方Bob用接收方Alic的公钥对自己的私钥进行加密，然后发送给Alice，Alic用自己的私钥解密即可得到发送方Bob的私钥，从而实现密钥交换功能。 graph LR A[Bob的私钥PRbob] -->|Bob的私钥PRbob|B[\"Bob的公钥环{PUalice,……}加密算法(如RSA)C=E(PUalice,PRbob)\"]; B -->|传输密文C|C[\"Alice的私钥PRalice解密算法(如RSA)PRbob=D(PRalice,C)\"]; C -->|PRbob|D[\"AliceBob的私钥PRbob\"]; 另外需要说明一下，Diffie-Hellman的密钥交换算法与此方法不同，如果你学过密码学，应该清楚其中的差异。并且并不是所有的公钥算法都支持加密/解密、数字签名和密钥交换功能，有的公钥算法只支持其中的一种或两种，下表列出部分公钥算法锁支持的应用。 算法 加密/解密 数字签名 密钥交换 RSA安全性建立在基于大素数分解是困难的 Y Y Y 椭圆曲线/ECC安全性建立在椭圆曲线对数问题之上(即由kP和P确定k是困难的) Y Y Y Diff-Hellman安全性建立在计算离散对数是很困难的 N Y Y DSS N Y N Hash函数(散列函数或摘要函数)Hash函数将可变长度的消息映射为固定长度的Hash值或消息摘要，常见的Hash算法有：MD2、MD4、MD5、SHA-1、SHA-224、SHA-256、SHA-384、SHA-512、HAVAL、HMAC、HMAC-MD5、HMAC-SHA1。对于给定的密码学Hash函数y=Hash(x)，要求如下两种情况再计算上不可行： 对给定的y，找到对应的x； 找到两个不同的x1和x2，使得Hash(x1)=Hash(x2)，具有抗碰撞性的特点。 Hash函数的应用 消息认证是用来验证消息完整性的一种机制或服务，消息认证确认收到的数据确实和发送时的一样(即防篡改)，并且还要确保发送方的身份是真实有效的的(即防冒充)。下图以对称加密算法为例，因为对称密钥K只有Bob和Alice才有，保证了发送方的合法有效性，同时比较C3与C是否相等，可以确定传输过程中是否被篡改过。 graph LR A[Bob输入明文P] -->|P|B[\"BobHash函数(如sha256)C=Hash(P)\"]; B -->|C|C[\"BobC1=P||C\"]; A -->|P|C; C -->|\"C1=P||C\"|D[\"Bob和Alice公用的密钥K对称加密算法(如DES)C2=E(K,C1)\"]; D -->|传输密文C2|E[\"Alice和Bob公用的密钥K对称解密算法(如DES)C1=D(K,C2)\"]; E -->|\"C1=P||C\"|F[\"Alice1.Hash函数(如sha256)C3=Hash(P)2.比较C3与C是否相等\"]; 数字签名(也名指纹或摘要)是一种认证机制，它使得消息的产生者可以添加一个起签名作用的码字，通过计算消息的Hash值并用产生者的私钥加密Hash值来生成签名，签名保证了消息和来源和完整性。下图最后一步比较C3与C如果不相等，认证失败，该图没有提供保密性，因为传输过程中只是将P和C1简单的连接在一起，并没有对C2进行加密，如果需要提供保密性，可以使用Alic的私钥对C2加密后再传输。 graph LR A[Bob输入明文P] -->|P|B[\"BobHash函数(如sha256)C=Hash(P)\"]; B -->|C|C[\"Bob的私钥PRbob加密算法(如RSA)C1=E(PRbob,C)\"]; C -->|C1|D[\"BobC2=P||C1\"]; A -->|P|D; D -->|传输C2|E[\"Alice的公钥环{PUbob,……}1.解密算法(如RSA)C=D(PUbob,C1)2.Hash函数(如sha256)C3=Hash(P)3.比较C3与C是否相等\"]; 用于产生单向口令文件，比如操作系统存储的都是口令的Hah值而不是口令本身，当用户输入口令时，计算其Hash值和之前存储的口令比对，这样即使操作系统被黑之后，也能保证用户口令的安全性。同样适用于入侵检测和病毒检测，如将你需要保护的文件的Hash值存储到安全系统中(比如只读设备中，不可修改也不可删除)，这样病毒入侵后只能修改文件而不能修改Hash值，于是可以通过重新计算文件的Hash值和之前保存的Hash值比对。 加密方式流密码典型的流密码是每次加密一个字节的密文，加密长度可以按需求设计，比如每次只加密一位或者大于一个字节的单元都行。实质上$Ci=Pi \\oplus K1i，Pi=Ci \\oplus K2i$，就是简单的异或，加密异或一次，解密再异或一次，即可恢复明文字节流。 graph LR A[\"Bob明文字节流P1~Pn\"] -->|\"P1~Pn\"|C[\"Bob加密函数Ci=E(K1i,Pi)\"]; B[\"Bob 由密钥K1控制的密钥流发生器K11～K1n其中K1i=K2i\"] -->|\"K11～K1n\"|C; C -->|\"传输密文C1～Cn\"|E[\"Alice解密函数Pi=D(K2i,Ci)\"]; D[\"Alice 由密钥K2控制的密钥流发生器K21～K2n其中K1i=K2i\"] -->|\"K21～K2n\"|E; E -->|\"明文字节流P1～Pn\"|F[\"Alice明文字节流P1~Pn\"]; 参考文献MathJax语法规则Mermaid语法规则Mermaid官方教程Mermaid Github仓库MathJax Github仓库常用加密算法概述HTTPS建立过程","categories":[{"name":"密码学与信息安全","slug":"密码学与信息安全","permalink":"https://icoty.github.io/categories/密码学与信息安全/"}],"tags":[{"name":"对称加密算法","slug":"对称加密算法","permalink":"https://icoty.github.io/tags/对称加密算法/"},{"name":"非对称加密算法/公钥算法","slug":"非对称加密算法-公钥算法","permalink":"https://icoty.github.io/tags/非对称加密算法-公钥算法/"},{"name":"Hash函数/散列函数/摘要函数","slug":"Hash函数-散列函数-摘要函数","permalink":"https://icoty.github.io/tags/Hash函数-散列函数-摘要函数/"},{"name":"消息认证","slug":"消息认证","permalink":"https://icoty.github.io/tags/消息认证/"},{"name":"流密码","slug":"流密码","permalink":"https://icoty.github.io/tags/流密码/"},{"name":"数字签名/指纹/消息摘要","slug":"数字签名-指纹-消息摘要","permalink":"https://icoty.github.io/tags/数字签名-指纹-消息摘要/"}]},{"title":"Nachos-Lab3-同步与互斥机制模块实现","slug":"nachos-3-4-Lab3","date":"2019-05-14T08:39:17.000Z","updated":"2019-05-25T09:29:34.000Z","comments":true,"path":"2019/05/14/nachos-3-4-Lab3/","link":"","permalink":"https://icoty.github.io/2019/05/14/nachos-3-4-Lab3/","excerpt":"","text":"源码获取https://github.com/icoty/nachos-3.4-Lab 内容一：总体概述本实习希望通过修改Nachos系统平台的底层源代码，达到“扩展同步机制，实现同步互斥实例”的目标。 内容二：任务完成情况任务完成列表（Y/N） Exercise1 Exercise2 Exercise3 Exercise4 Challenge1 Challenge2 Challenge3 第一部分 Y Y Y Y Y Y N 具体Exercise的完成情况Exercise1 调研调研Linux或Windows中采用的进程/线程调度算法。具体内容见课堂要求。 同步是指用于实现控制多个进程按照一定的规则或顺序访问某些系统资源的机制，进程间的同步方式有共享内存，套接字，管道，信号量，消息队列，条件变量；线程间的同步有套接字，消息队列，全局变量，条件变量，信号量。 互斥是指用于实现控制某些系统资源在任意时刻只能允许一个进程访问的机制。互斥是同步机制中的一种特殊情况。进程间的互斥方式有锁，信号量，条件变量；线程间的互斥方式有信号量，锁，条件变量。此外，通过硬件也能实现同步与互斥。 linux内核中提供的同步机制 原子操作 自旋锁 读写自旋锁 信号量 读写信号量 互斥量 完成变量 大内核锁 顺序锁 禁止抢占 顺序和屏障 Exercise2 源代码阅读code/threads/synch.h和code/threads/synch.cc：Condition和Lock仅仅声明了未定义；Semaphore既声明又定义了。 Semaphore有一个初值和一个等待队列，提供P、V操作： P操作：当value等于0时，将当前运行线程放入线程等待队列，当前进程进入睡眠状态，并切换到其他线程运行；当value大于0时，value–。 V操作：如果线程等待队列中有等待该信号量的线程，取出其中一个将其设置成就绪态，准备运行，value++。 Lock：Nachos中没有给出锁机制的实现，接口有获得锁(Acquire)和释放锁(Release)，他们都是原子操作。 Acquire：当锁处于BUSY态，进入睡眠状态。当锁处于FREE态，当前进程获得该锁，继续运行。 Release：释放锁（只能由拥有锁的线程才能释放锁），将锁的状态设置为FREE态，如果有其他线程等待该锁，将其中的一个唤醒，进入就绪态。 Condition：条件变量同信号量、锁机制不一样，条件变量没值。当一个线程需要的某种条件没有得到满足时，可以将自己作为一个等待条件变量的线程插入所有等待该条件变量的队列，只要条件一旦得到满足，该线程就会被唤醒继续运行。条件变量总是和锁机制一起使。主要接口Wait、Signal、BroadCast，这三个操作必须在当前线程获得一个锁的前提下，而且所有对一个条件变量进行的操作必须建立在同一个锁的前提下。 Wait(Lock *conditionLock)：线程等待在条件变量上，把线程放入条件变量的等待队列上。 Signal(Lock *conditionLock)：从条件变量的等待队列中唤醒一个等待该条件变量的线程。 BroadCast(Lock *conditionLock)：唤醒所有等待该条件变量的线程。 code/threads/synchlist.h和code/threads/synchlist.cc：利用锁、条件变量实现的一个消息队列，使多线程达到互斥访问和同步通信的目的，类内有一个Lock和List成员变量。提供了对List的Append()，Remove()和Mapcar()操作。每个操作都要先获得该锁，然后才能对List进行相应的操作。 Exercise3 实现锁和条件变量可以使用sleep和wakeup两个原语操作（注意屏蔽系统中断），也可以使用Semaphore作为唯一同步原语（不必自己编写开关中断的代码）。 这里选择用1值信号量实现锁功能，Lock添加成员变量lock和owner，请求锁和释放锁都必须关中断，Condition添加一个成员变量queue，用于存放所有等待在该条件变量上的线程。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899// synch.h Lock声明部分class Lock &#123;……private: char* name; // for debugging // add by yangyu Semaphore *lock; Thread* owner;&#125;;class Condition &#123;……private: char* name; // add by yangyu List* queue;&#125;;// synch.cc Lock定义部分Lock::Lock(char* debugName) :lock(new Semaphore(\"lock\", 1)),name(debugName),owner(NULL)&#123;&#125;Lock::~Lock() &#123; delete lock;&#125;bool Lock::isHeldByCurrentThread()&#123; return currentThread == owner;&#125;void Lock::Acquire() &#123; IntStatus prev = interrupt-&gt;SetLevel(IntOff); lock-&gt;P(); owner = currentThread; (void)interrupt-&gt;SetLevel(prev);&#125;void Lock::Release() &#123; IntStatus prev = interrupt-&gt;SetLevel(IntOff); ASSERT(currentThread == owner); lock-&gt;V(); owner = NULL; (void)interrupt-&gt;SetLevel(prev);&#125;// synch.cc Condition定义部分Condition::Condition(char* debugName):name(debugName),queue(new List)&#123; &#125;Condition::~Condition()&#123; &#125;void Condition::Wait(Lock* conditionLock) &#123; //ASSERT(FALSE); // 关中断 IntStatus prev = interrupt-&gt;SetLevel(IntOff); // 锁和信号量不同，谁加锁必须由谁解锁，因此做下判断 ASSERT(conditionLock-&gt;isHeldByCurrentThread()); // 进入睡眠前把锁的权限释放掉，然后放到等待队列，直到被唤醒时重新征用锁 conditionLock-&gt;Release(); queue-&gt;Append(currentThread); currentThread-&gt;Sleep(); conditionLock-&gt;Acquire(); (void)interrupt-&gt;SetLevel(prev);&#125;void Condition::Signal(Lock* conditionLock) &#123; IntStatus prev = interrupt-&gt;SetLevel(IntOff); ASSERT(conditionLock-&gt;isHeldByCurrentThread()); if(!queue-&gt;IsEmpty()) &#123; // 唤醒一个等待的线程，挂入倒就绪队列中 Thread* next = (Thread*)queue-&gt;Remove(); scheduler-&gt;ReadyToRun(next); &#125; (void)interrupt-&gt;SetLevel(prev);&#125;void Condition::Broadcast(Lock* conditionLock) &#123; IntStatus prev = interrupt-&gt;SetLevel(IntOff); ASSERT(conditionLock-&gt;isHeldByCurrentThread()); // 唤醒等待在该条件变量上的所有线程 while(!queue-&gt;IsEmpty()) &#123; Signal(conditionLock); &#125; (void)interrupt-&gt;SetLevel(prev);&#125; Exercise4 实现同步互斥实例基于Nachos中的信号量、锁和条件变量，采用两种方式实现同步和互斥机制应用（其中使用条件变量实现同步互斥机制为必选题目）。具体可选择“生产者-消费者问题”、“读者-写者问题”、“哲学家就餐问题”、“睡眠理发师问题”等。（也可选择其他经典的同步互斥问题）。 生产者-消费者问题(Condition实现)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121// threadtest.cc// 条件变量实现生产者消费者问题Condition* condc = new Condition(\"ConsumerCondition\");Condition* condp = new Condition(\"ProducerCondition\");Lock* pcLock = new Lock(\"producerConsumerLock\");int shareNum = 0; // 共享内容，生产+1，消费-1，互斥访问// lab3 条件变量实现生产者消费者问题void Producer1(int val)&#123; while(1)&#123; pcLock-&gt;Acquire(); // 缓冲区已满则等待在条件变量上，停止生产，等待消费后再生产 while(shareNum &gt;= N)&#123; printf(\"Product alread full:[%d],threadId:[%d],wait consumer.\\n\",shareNum,currentThread-&gt;getThreadId()); condp-&gt;Wait(pcLock); &#125; printf(\"name:[%s],threadId:[%d],before:[%d],after:[%d]\\n\",currentThread-&gt;getName(),currentThread-&gt;getThreadId(),shareNum,shareNum+1); ++shareNum; // 生产一个通知可消费，唤醒一个等待在condc上的消费者 condc-&gt;Signal(pcLock); pcLock-&gt;Release(); sleep(val); &#125;&#125;void Customer1(int val)&#123; while(1)&#123; pcLock-&gt;Acquire(); // 为零表示已经消费完毕,等待在条件变量上，等待生产后再消费 while(shareNum &lt;= 0)&#123; printf(\"--&gt;Product alread empty:[%d],threadId:[%d],wait producer.\\n\",shareNum,currentThread-&gt;getThreadId()); condc-&gt;Wait(pcLock); &#125; printf(\"--&gt;name:[%s],threadId:[%d],before:[%d],after:[%d]\\n\",currentThread-&gt;getName(),currentThread-&gt;getThreadId(),shareNum,shareNum-1); --shareNum; // 消费一个后通知生产者缓冲区不为满，可以生产 condp-&gt;Signal(pcLock); pcLock-&gt;Release(); //sleep(val); &#125;&#125;void ThreadProducerConsumerTest1()&#123; DEBUG('t', \"Entering ThreadProducerConsumerTest1\"); // 两个生产者循环生产 Thread* p1 = new Thread(\"Producer1\"); Thread* p2 = new Thread(\"Producer2\"); p1-&gt;Fork(Producer1, 1); p2-&gt;Fork(Producer1, 3); // 两个消费者循环消费 Thread* c1 = new Thread(\"Consumer1\"); Thread* c2 = new Thread(\"Consumer2\"); c1-&gt;Fork(Customer1, 1); c2-&gt;Fork(Customer1, 2);&#125;void ThreadTest()&#123; switch (testnum) &#123; case 1: ThreadTest1(); break; case 2: ThreadCountLimitTest(); break; case 3: ThreadPriorityTest(); break; case 4: ThreadProducerConsumerTest(); break; case 5: ThreadProducerConsumerTest1(); break; case 6: barrierThreadTest(); break; case 7: readWriteThreadTest(); break; default: printf(\"No test specified.\\n\"); break; &#125;&#125;// 运行结果，需要-rs，否则可能没有中断发生，永远是一个线程在运行// 通过结果可以明确看出生产前和生产后，消费前和消费后的数值变化// 可以通过修改Producer1和Consumer1内的sleep(val)来调整不同的速度// 当生产满了会停止生产，消费完了也会停止消费root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -rs -q 5name:[Producer1],threadId:[1],before:[0],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer1],threadId:[1],before:[1],after:[2]--&gt;name:[Consumer2],threadId:[4],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer1],threadId:[1],before:[1],after:[2]--&gt;name:[Consumer2],threadId:[4],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]--&gt;name:[Consumer2],threadId:[4],before:[1],after:[0]name:[Producer1],threadId:[1],before:[0],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]name:[Producer1],threadId:[1],before:[2],after:[3]--&gt;name:[Consumer2],threadId:[4],before:[3],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]name:[Producer1],threadId:[1],before:[2],after:[3]--&gt;name:[Consumer2],threadId:[4],before:[3],after:[2]--&gt;name:[Consumer1],threadId:[3],before:[2],after:[1]name:[Producer2],threadId:[2],before:[1],after:[2]^CCleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# 生产者-消费者问题(Semaphore实现)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485// threadtest.cc// 信号量解决生产者消费者问题#define N 1024 // 缓冲区大小Semaphore* empty = new Semaphore(\"emptyBuffer\", N);Semaphore* mutex = new Semaphore(\"lockSemaphore\", 1);Semaphore* full = new Semaphore(\"fullBuffer\", 0);int msgQueue = 0;void Producer(int val)&#123; while(1) &#123; empty-&gt;P(); mutex-&gt;P(); if(msgQueue &gt;= N)&#123; // 已经满了则停止生产 printf(\"--&gt;Product alread full:[%d],wait consumer.\",msgQueue); &#125;else&#123; printf(\"--&gt;name:[%s],threadId:[%d],before:[%d],after:[%d]\\n\",\\ currentThread-&gt;getName(),currentThread-&gt;getThreadId(),msgQueue,msgQueue+1); ++msgQueue; &#125; mutex-&gt;V(); full-&gt;V(); sleep(val); // 休息下再生产 &#125;&#125;void Customer(int val)&#123; while(1) &#123; full-&gt;P(); mutex-&gt;P(); if(msgQueue &lt;= 0)&#123; printf(\"Product alread empty:[%d],wait Producer.\",msgQueue); &#125;else&#123; printf(\"name:[%s] threadId:[%d],before:[%d],after:[%d]\\n\",\\ currentThread-&gt;getName(),currentThread-&gt;getThreadId(),msgQueue,msgQueue-1); --msgQueue; &#125; mutex-&gt;V(); empty-&gt;V(); sleep(val); // 休息下再消费 &#125;&#125;void ThreadProducerConsumerTest()&#123; DEBUG('t', \"Entering ThreadProducerConsumerTest\"); // 两个生产者 Thread* p1 = new Thread(\"Producer1\"); Thread* p2 = new Thread(\"Producer2\"); p1-&gt;Fork(Producer, 1); p2-&gt;Fork(Producer, 3); // 两个消费者，可以关掉一个消费者，查看生产速率和消费速率的变化 Thread* c1 = new Thread(\"Consumer1\"); //Thread* c2 = new Thread(\"Consumer2\"); c1-&gt;Fork(Customer, 1); //c2-&gt;Fork(Customer, 2);&#125;// 通过结果可以明确看出生产前和生产后，消费前和消费后的数值变化// 可以通过修改Producer和Consumer内的sleep(val)来调整不同的速度// 当生产满了会停止生产，消费完了也会停止消费root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -rs -q 4--&gt;name:[Producer1],threadId:[1],before:[0],after:[1]--&gt;name:[Producer2],threadId:[2],before:[1],after:[2]name:[Consumer1] threadId:[3],before:[2],after:[1]--&gt;name:[Producer1],threadId:[1],before:[1],after:[2]--&gt;name:[Producer2],threadId:[2],before:[2],after:[3]--&gt;name:[Producer1],threadId:[1],before:[3],after:[4]name:[Consumer1] threadId:[3],before:[4],after:[3]--&gt;name:[Producer2],threadId:[2],before:[3],after:[4]name:[Consumer1] threadId:[3],before:[4],after:[3]--&gt;name:[Producer1],threadId:[1],before:[3],after:[4]--&gt;name:[Producer2],threadId:[2],before:[4],after:[5]name:[Consumer1] threadId:[3],before:[5],after:[4]--&gt;name:[Producer1],threadId:[1],before:[4],after:[5]--&gt;name:[Producer2],threadId:[2],before:[5],after:[6]--&gt;name:[Producer1],threadId:[1],before:[6],after:[7]--&gt;name:[Producer2],threadId:[2],before:[7],after:[8]name:[Consumer1] threadId:[3],before:[8],after:[7]^CCleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# Challenge1 实现barrier(至少选做一个Challenge)可以使用Nachos 提供的同步互斥机制（如条件变量）来实现barrier，使得当且仅当若干个线程同时到达某一点时方可继续执行。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// threadtest.cc// 条件变量实现barrierCondition* barrCond = new Condition(\"BarrierCond\");Lock* barrLock = new Lock(\"BarrierLock\");int barrierCnt = 0;// 当且仅当barrierThreadNum个线程同时到达时才能往下运行const int barrierThreadNum = 5; void barrierFun(int num)&#123; /*while(1)*/ &#123; barrLock-&gt;Acquire(); ++barrierCnt; if(barrierCnt == barrierThreadNum)&#123; // 最后一个线程到达后判断，条件满足则发送一个广播信号 // 唤醒等待在该条件变量上的所有线程 printf(\"threadName:[%s%d],barrierCnt:[%d],needCnt:[%d],Broadcast.\\n\",\\ currentThread-&gt;getName(),num,barrierCnt,barrierThreadNum); barrCond-&gt;Broadcast(barrLock); barrLock-&gt;Release(); &#125;else&#123; // 每一个线程都执行判断，若条件不满足，线程等待在条件变量上 printf(\"threadName:[%s%d],barrierCnt:[%d],needCnt:[%d],Wait.\\n\",\\ currentThread-&gt;getName(),num,barrierCnt,barrierThreadNum); barrCond-&gt;Wait(barrLock); barrLock-&gt;Release(); &#125; printf(\"threadName:[%s%d],continue to run.\\n\", currentThread-&gt;getName(),num); &#125;&#125;void barrierThreadTest()&#123; DEBUG('t', \"Entering barrierThreadTest\"); for(int i = 0; i &lt; barrierThreadNum; ++i)&#123; Thread* t = new Thread(\"barrierThread\"); t-&gt;Fork(barrierFun,i+1); &#125;&#125;// 运行结果，当第五个线程进入后判断条件满足，唤醒所有线程root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -rs -q 6threadName:[barrierThread1],barrierCnt:[1],needCnt:[5],Wait.threadName:[barrierThread2],barrierCnt:[2],needCnt:[5],Wait.threadName:[barrierThread3],barrierCnt:[3],needCnt:[5],Wait.threadName:[barrierThread4],barrierCnt:[4],needCnt:[5],Wait.threadName:[barrierThread5],barrierCnt:[5],needCnt:[5],Broadcast.threadName:[barrierThread5],continue to run.threadName:[barrierThread2],continue to run.threadName:[barrierThread1],continue to run.threadName:[barrierThread4],continue to run.threadName:[barrierThread3],continue to run.No threads ready or runnable, and no pending interrupts.Assuming the program completed.Machine halting!Ticks: total 814, idle 4, system 810, user 0Disk I/O: reads 0, writes 0Console I/O: reads 0, writes 0Paging: faults 0Network I/O: packets received 0, sent 0Cleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# Challenge2 实现read/write lock基于Nachos提供的lock(synch.h和synch.cc)，实现read/write lock。使得若干线程可以同时读取某共享数据区内的数据，但是在某一特定的时刻，只有一个线程可以向该共享数据区写入数据。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// threadtest.cc// Lab3 锁实现读者写者问题int rCnt = 0; // 记录读者数量Lock* rLock = new Lock(\"rlock\");// 必须用信号量，不能用锁，因为锁只能由加锁的线程解锁Semaphore* wLock = new Semaphore(\"wlock\",1); int bufSize = 0;// Lab3 锁实现读者写者问题void readFunc(int num)&#123; while(1) &#123; rLock-&gt;Acquire(); ++rCnt; // 如果是第一个读者进入，需要竞争1值信号量wLock，竞争成功才能进入临界区 // 一旦竞争到wLock，由最后一个读者出临界区后释放，保证了读者优先 if(rCnt == 1)&#123; wLock-&gt;P(); &#125; rLock-&gt;Release(); if(0 == bufSize)&#123; // 没有数据可读 printf(\"threadName:[%s],bufSize:[%d],current not data.\\n\",currentThread-&gt;getName(),bufSize); &#125;else&#123; // 读取数据 printf(\"threadName:[%s],bufSize:[%d],exec read operation.\\n\",currentThread-&gt;getName(),bufSize); &#125; rLock-&gt;Acquire(); --rCnt; // 最后一个读者释放wLock if(rCnt == 0)&#123; wLock-&gt;V(); &#125; rLock-&gt;Release(); currentThread-&gt;Yield(); sleep(num); &#125;&#125;void writeFunc(int num)&#123; while(1) &#123; wLock-&gt;P(); ++bufSize; printf(\"writerThread:[%s],before:[%d],after:[%d]\\n\", currentThread-&gt;getName(), bufSize, bufSize+1); wLock-&gt;V(); currentThread-&gt;Yield(); sleep(num); &#125;&#125;void readWriteThreadTest()&#123; DEBUG('t', \"Entering readWriteThreadTest\"); Thread * r1 = new Thread(\"read1\"); Thread * r2 = new Thread(\"read2\"); Thread * r3 = new Thread(\"read3\"); Thread * w1 = new Thread(\"write1\"); Thread * w2 = new Thread(\"write2\"); // 3个读者2个写者 r1-&gt;Fork(readFunc,1); w1-&gt;Fork(writeFunc,1); r2-&gt;Fork(readFunc,1); w2-&gt;Fork(writeFunc,1); r3-&gt;Fork(readFunc,1);&#125;// 运行结果，第一个读者进入无数据可读// 可以发现读操作比写操作多// 一旦开始读，就要等所有线程读取完毕后，写线程才进入root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -rs -q 7threadName:[read1],Val:[0],current not data.writerThread:[write1],before:[0],after:[1]writerThread:[write2],before:[1],after:[2]writerThread:[write1],before:[2],after:[3]writerThread:[write2],before:[3],after:[4]threadName:[read2],readVal:[4],exec read operation.threadName:[read1],readVal:[4],exec read operation.threadName:[read3],readVal:[4],exec read operation.writerThread:[write1],before:[4],after:[5]threadName:[read2],readVal:[5],exec read operation.threadName:[read3],readVal:[5],exec read operation.threadName:[read2],readVal:[5],exec read operation.threadName:[read3],readVal:[5],exec read operation.threadName:[read2],readVal:[5],exec read operation.threadName:[read3],readVal:[5],exec read operation.threadName:[read1],readVal:[5],exec read operation.writerThread:[write2],before:[5],after:[6]writerThread:[write1],before:[6],after:[7]^CCleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# 内容三：遇到的困难以及解决方法困难1刚开始没有加-rs参数，导致永远都只有一个线程在运行，原因是没有中断发生，运行的线程永远在执行循环体，加-rs参数，会在一个固定的时间短内发一个时钟中断，然后调度其他线程运行。 内容四：收获及感想可以说实际操作后，对信号量，条件变量的应用更加清晰了。 内容五：对课程的意见和建议暂无。 内容六：参考文献https://blog.csdn.net/FreeeLinux/article/details/54267446","categories":[{"name":"同步机制","slug":"同步机制","permalink":"https://icoty.github.io/categories/同步机制/"},{"name":"互斥机制","slug":"互斥机制","permalink":"https://icoty.github.io/categories/互斥机制/"}],"tags":[{"name":"条件变量实现","slug":"条件变量实现","permalink":"https://icoty.github.io/tags/条件变量实现/"},{"name":"锁实现","slug":"锁实现","permalink":"https://icoty.github.io/tags/锁实现/"},{"name":"信号量","slug":"信号量","permalink":"https://icoty.github.io/tags/信号量/"},{"name":"Nachos-3.4","slug":"Nachos-3-4","permalink":"https://icoty.github.io/tags/Nachos-3-4/"},{"name":"生产者消费者问题","slug":"生产者消费者问题","permalink":"https://icoty.github.io/tags/生产者消费者问题/"},{"name":"读者写者问题","slug":"读者写者问题","permalink":"https://icoty.github.io/tags/读者写者问题/"},{"name":"Barrier实现","slug":"Barrier实现","permalink":"https://icoty.github.io/tags/Barrier实现/"}]},{"title":"Nachos-Lab2-线程调度模块实现","slug":"nachos-3-4-Lab2","date":"2019-05-14T04:57:24.000Z","updated":"2019-05-15T08:53:28.000Z","comments":true,"path":"2019/05/14/nachos-3-4-Lab2/","link":"","permalink":"https://icoty.github.io/2019/05/14/nachos-3-4-Lab2/","excerpt":"","text":"源码获取https://github.com/icoty/nachos-3.4-Lab 内容一：总体概述本实习希望通过修改Nachos系统平台的底层源代码，达到“扩展调度算法”的目标。本次实验主要是要理解Timer、Scheduler和Interrupt之间的关系，从而理解线程之间是如何进行调度的。 内容二：任务完成情况任务完成列表（Y/N） Exercise1 Exercise2 Exercise3 Challenge1 第一部分 Y Y Y Y 具体Exercise的完成情况Exercise1 调研调研Linux或Windows中采用的进程/线程调度算法。具体内容见课堂要求。 linux-4.19.23进程调度策略：SCHED_OTHER分时调度策略，SCHED_FIFO实时调度策略（先到先服务），SCHED_RR实时调度策略（时间片轮转）。 RR调度和FIFO调度的进程属于实时进程，以分时调度的进程是非实时进程。 当实时进程准备就绪后，如果当前cpu正在运行非实时进程，则实时进程立即抢占非实时进程。 RR进程和FIFO进程都采用实时优先级做为调度的权值标准，RR是FIFO的一个延伸。FIFO时，如果两个进程的优先级一样，则这两个优先级一样的进程具体执行哪一个是由其在队列中的位置决定的，这样导致一些不公正性(优先级是一样的，为什么要让你一直运行?)，如果将两个优先级一样的任务的调度策略都设为RR，则保证了这两个任务可以循环执行，保证了公平。 内核代码：内核为每个cpu维护一个进程就绪队列，cpu只调度由其维护的队列上的进程： vi linux-4.19.23/kernel/sched/core.c：123456……#define CREATE_TRACE_POINTS#include &lt;trace/events/sched.h&gt;DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);…… ​ vi linux-4.19.23/kernel/sched/sched.h：123456789101112131415161718192021222324252627282930313233/* * This is the main, per-CPU runqueue data structure. * * Locking rule: those places that want to lock multiple runqueues * (such as the load balancing or the thread migration code), lock * acquire operations must be ordered by ascending &amp;runqueue. */struct rq &#123; /* runqueue lock: */ raw_spinlock_t lock; // 锁保证互斥访问runqueue …… struct cfs_rq cfs; // 所有普通进程的集合，采用cfs调度策略 struct rt_rq rt; // 所有实时进程的集合，采用实时调度策略 struct dl_rq dl; // struct dl_rq空闲进程集合 ……&#125;;// cfs_rq就绪队列是一棵红黑树。/* CFS-related fields in a runqueue */struct cfs_rq &#123; …… struct rb_root_cached tasks_timeline; // 红黑树的树根 /* * 'curr' points to currently running entity on this cfs_rq. * It is set to NULL otherwise (i.e when none are currently running). */ struct sched_entity *curr; // 指向当前正运行的进程 struct sched_entity *next; // 指向将被唤醒的进程 struct sched_entity *last; // 指向唤醒next进程的进程 struct sched_entity *skip; ……&#125;; ​ vi linux-4.19.23/include/linux/sched.h：实时进程调度实体struct sched_rt_entity，双向链表组织形式；空闲进程调度实体struct sched_dl_entity，红黑树组织形式；普通进程的调度实体sched_entity，每个进程描述符中均包含一个该结构体变量，该结构体有两个作用： 包含有进程调度的信息（比如进程的运行时间，睡眠时间等等，调度程序参考这些信息决定是否调度进程）； 使用该结构体来组织进程，struct rb_node类型结构体变量run_node是红黑树节点，struct sched_entity调度实体将被组织成红黑树的形式，同时意味着普通进程也被组织成红黑树的形式。parent指向了当前实体的上一级实体，cfs_rq指向了该调度实体所在的就绪队列。my_q指向了本实体拥有的就绪队列（调度组），该调度组（包括组员实体）属于下一个级别，和本实体不在同一个级别，该调度组中所有成员实体的parent域指向了本实体，depth代表了此队列（调度组）的深度，每个调度组都比其parent调度组深度大1。内核依赖my_q域实现组调度。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546……// 普通进程的调度实体sched_entity，使用红黑树组织struct sched_entity &#123; /* For load-balancing: */ struct load_weight load; unsigned long runnable_weight; struct rb_node run_node; // 红黑树节点 struct list_head group_node; unsigned int on_rq; ……#ifdef CONFIG_FAIR_GROUP_SCHED int depth; struct sched_entity *parent; // 当前节点的父节点 /* rq on which this entity is (to be) queued: */ struct cfs_rq *cfs_rq; // 当前节点所在的就绪队列 /* rq \"owned\" by this entity/group: */ struct cfs_rq *my_q;#endif ……&#125;;// 实时进程调度实体，采用双向链表组织struct sched_rt_entity &#123; struct list_head run_list; // 链表组织 unsigned long timeout; unsigned long watchdog_stamp; unsigned int time_slice; unsigned short on_rq; unsigned short on_list; struct sched_rt_entity *back;#ifdef CONFIG_RT_GROUP_SCHED struct sched_rt_entity *parent; /* rq on which this entity is (to be) queued: */ struct rt_rq *rt_rq; // 当前节点所在的就绪队列 /* rq \"owned\" by this entity/group: */ struct rt_rq *my_q;#endif&#125; __randomize_layout;// 空闲进程调度实体，采用红黑树组织struct sched_dl_entity &#123; struct rb_node rb_node; ……&#125;;…… ​ vi linux-4.19.23/kernel/sched/sched.h：内核声明了一个调度类sched_class的结构体类型，用来实现不同的调度策略，可以看到该结构体成员都是函数指针，这些指针指向的函数就是调度策略的具体实现，所有和进程调度有关的函数都直接或者间接调用了这些成员函数，来实现进程调度。此外，每个进程描述符中都包含一个指向该结构体类型的指针sched_class，指向了所采用的调度类。 12345678910111213……struct sched_class &#123; const struct sched_class *next; void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags); void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags); void (*yield_task) (struct rq *rq); bool (*yield_to_task)(struct rq *rq, struct task_struct *p, bool preempt); void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags); ……&#125;;…… Exercise2 源代码阅读code/threads/scheduler.h和code/threads/scheduler.cc：scheduler类是nachos中的进程调度器，维护了一个挂起的中断队列，通过FIFO进行调度。 void ReadyToRun(Thread* thread)；设置线程状态为READY，并放入就绪队列readyList。 Thread* FindNextToRun(int source); 从就绪队列中取出下一个上CPU的线程，实现基于优先级的抢占式调度和FIFO调度。 void Run(Thread* nextThread); 把下CPU的线程的寄存器和堆栈信息从CPU保存到线程本身的寄存器数据结构中， 执行线程切换，把上CPU的线程的寄存器和堆栈信息从线程本身的寄存器中拷贝到CPU的寄存器中，运行新线程。 code/threads/switch.s：switch.s模拟内容是汇编代码，负责CPU上进程的切换。切换过程中，首先保存当前进程的状态，然后恢复新运行进程的状态，之后切换到新进程的栈空间，开始运行新进程。 code/machine/timer.h和code/machine/timer.cc：Timer类用以模拟硬件的时间中断。在TimerExired中，会调用TimeOfNextInterrupt，计算出下次时间中断的时间，并将中断插入中断队列中。初始化时会调用TimerExired，然后每次中断处理函数中都会调用一次TimerExired，从而时间系统时间一步步向前走。需要说明的是，在运行nachos时加入-rs选项，会初始化一个随机中断的Timer。当然你也可以自己声明一个非随机的Timer，每隔固定的时间片执行中断。时间片大小的定义位于ststs.h中，每次开关中断会调用OneTick()，当Ticks数目达到时间片大小时，会出发一次时钟中断。 Exercise3 线程调度算法扩展扩展线程调度算法，实现基于优先级的抢占式调度算法。 思路：更改Thread类，加入priority成员变量，同时更改初始化函数对其初始化，并完成对应的set和get函数。scheduler中的FindNextToRun负责找到下一个运行的进程，默认是FIFO，找到队列最开始时的线程返回。我们现在要实现的是根据优先级来返回，仅需将插入readyList队列的方法按照优先级从高到低顺序插入SortedInsert，那么插入时会维护队列中的Thread按照优先级排序，每次依旧从头取出第一个，即为优先级最高的队列。抢占式调度则需要在每次中断发生时尝试进行进程切换，如果有优先级更高的进程，则运行高优先级进程。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125// 基于优先级的可抢占式调度策略和FIFO调度策略Thread * Scheduler::FindNextToRun (bool bySleep)&#123; // called by threadsleep，直接调度，不用判断时间片 if(bySleep)&#123; lastSwitchTick = stats-&gt;systemTicks; return (Thread *)readyList-&gt;SortedRemove(NULL); // 与Remove()等价，都是从队头取 &#125;else&#123; int ticks = stats-&gt;systemTicks - lastSwitchTick; // 这里设置了运行的最短时间TimerSlice，防止频繁切换消耗CPU资源 // 测试优先级抢占调度时需要屏蔽这句，因为调用Yield()的线程运行时间很短 // 会直接返回NULL /*if(ticks &lt; TimerSlice)&#123; // 不用切换 return NULL; &#125;else*/&#123; if(readyList-&gt;IsEmpty())&#123; return NULL; &#125; Thread * next = (Thread *)readyList-&gt;SortedRemove(NULL);// 基于优先级可抢占调度策略,自己添加的宏，Makefile编译添加： -DSCHED_PRIORITY#ifdef SCHED_PRIORITY // nextThread优先级高于当前线程则切换，否则不切换 if(next-&gt;getPriority() &lt; currentThread-&gt;getPriority())&#123; lastSwitchTick = stats-&gt;systemTicks; return next; &#125;else&#123; readyList-&gt;SortedInsert(next, next-&gt;getPriority()); return NULL; &#125;#else // FIFO策略需要取消Makefile编译选项：-DSCHED_PRIORITY lastSwitchTick = stats-&gt;systemTicks; return next;#endif &#125; &#125;&#125;// 线程主动让出cpu,在FIFO调度策略下能够看到多个线程按顺序运行void SimpleThread(int which)&#123; for (int num = 0; num &lt; 5; num++) &#123; int ticks = stats-&gt;systemTicks - scheduler-&gt;getLastSwitchTick(); printf(\"userId=%d,threadId=%d,prio=%d,loop:%d,lastSwitchTick=%d,systemTicks=%d,usedTicks=%d,TimerSlice=%d\\n\",currentThread-&gt;getUserId(),currentThread-&gt;getThreadId(),currentThread-&gt;getPriority(),num,scheduler-&gt;getLastSwitchTick(),stats-&gt;systemTicks,ticks,TimerSlice); // 时间片轮转算法，判断时间片是否用完， // 如果用完主动让出cpu，针对nachos内核线程算法 /*if(ticks &gt;= TimerSlice)&#123; //printf(\"threadId=%d Yield\\n\",currentThread-&gt;getThreadId()); currentThread-&gt;Yield(); &#125;*/ // 非抢占模式下，多个线程同时执行该接口的话，会交替执行，交替让出cpu // 基于优先级抢占模式下，优先级高的线程运行结束后才调度低优先级线程 currentThread-&gt;Yield(); &#125;&#125;threadtest.cc:// 创建四个线程，加上主线程共五个，优先值越小优先级越高void ThreadPriorityTest()&#123; Thread* t1 = new Thread(\"forkThread1\", 1); printf(\"--&gt;name=%s,threadId=%d\\n\",t1-&gt;getName(),t1-&gt;getThreadId()); t1-&gt;Fork(SimpleThread, (void*)1); Thread* t2 = new Thread(\"forkThread2\", 2); printf(\"--&gt;name=%s,threadId=%d\\n\",t2-&gt;getName(),t2-&gt;getThreadId()); t2-&gt;Fork(SimpleThread, (void*)2); Thread* t3 = new Thread(\"forkThread3\", 3); printf(\"--&gt;name=%s,threadId=%d\\n\",t3-&gt;getName(),t3-&gt;getThreadId()); t3-&gt;Fork(SimpleThread, (void*)3); Thread* t4 = new Thread(\"forkThread4\", 4); printf(\"--&gt;name=%s,threadId=%d\\n\",t4-&gt;getName(),t4-&gt;getThreadId()); t4-&gt;Fork(SimpleThread, (void*)4); currentThread-&gt;Yield(); SimpleThread(0);&#125;// 运行结果，优先级1最高，最先执行完，其次是优先为2的线程，直到所有线程结束root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -q 3--&gt;name=forkThread1,threadId=1--&gt;name=forkThread2,threadId=2--&gt;name=forkThread3,threadId=3--&gt;name=forkThread4,threadId=4userId=0,threadId=1,prio=1,loop:0,lastSwitchTick=50,systemTicks=60,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=1,loop:1,lastSwitchTick=50,systemTicks=70,usedTicks=20,TimerSlice=30userId=0,threadId=1,prio=1,loop:2,lastSwitchTick=50,systemTicks=80,usedTicks=30,TimerSlice=30userId=0,threadId=1,prio=1,loop:3,lastSwitchTick=50,systemTicks=90,usedTicks=40,TimerSlice=30userId=0,threadId=1,prio=1,loop:4,lastSwitchTick=50,systemTicks=100,usedTicks=50,TimerSlice=30userId=0,threadId=2,prio=2,loop:0,lastSwitchTick=110,systemTicks=120,usedTicks=10,TimerSlice=30userId=0,threadId=2,prio=2,loop:1,lastSwitchTick=110,systemTicks=130,usedTicks=20,TimerSlice=30userId=0,threadId=2,prio=2,loop:2,lastSwitchTick=110,systemTicks=140,usedTicks=30,TimerSlice=30userId=0,threadId=2,prio=2,loop:3,lastSwitchTick=110,systemTicks=150,usedTicks=40,TimerSlice=30userId=0,threadId=2,prio=2,loop:4,lastSwitchTick=110,systemTicks=160,usedTicks=50,TimerSlice=30userId=0,threadId=3,prio=3,loop:0,lastSwitchTick=170,systemTicks=180,usedTicks=10,TimerSlice=30userId=0,threadId=3,prio=3,loop:1,lastSwitchTick=170,systemTicks=190,usedTicks=20,TimerSlice=30userId=0,threadId=3,prio=3,loop:2,lastSwitchTick=170,systemTicks=200,usedTicks=30,TimerSlice=30userId=0,threadId=3,prio=3,loop:3,lastSwitchTick=170,systemTicks=210,usedTicks=40,TimerSlice=30userId=0,threadId=3,prio=3,loop:4,lastSwitchTick=170,systemTicks=220,usedTicks=50,TimerSlice=30userId=0,threadId=4,prio=4,loop:0,lastSwitchTick=230,systemTicks=240,usedTicks=10,TimerSlice=30userId=0,threadId=4,prio=4,loop:1,lastSwitchTick=230,systemTicks=250,usedTicks=20,TimerSlice=30userId=0,threadId=4,prio=4,loop:2,lastSwitchTick=230,systemTicks=260,usedTicks=30,TimerSlice=30userId=0,threadId=4,prio=4,loop:3,lastSwitchTick=230,systemTicks=270,usedTicks=40,TimerSlice=30userId=0,threadId=4,prio=4,loop:4,lastSwitchTick=230,systemTicks=280,usedTicks=50,TimerSlice=30userId=0,threadId=0,prio=6,loop:0,lastSwitchTick=290,systemTicks=300,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=6,loop:1,lastSwitchTick=290,systemTicks=310,usedTicks=20,TimerSlice=30userId=0,threadId=0,prio=6,loop:2,lastSwitchTick=290,systemTicks=320,usedTicks=30,TimerSlice=30userId=0,threadId=0,prio=6,loop:3,lastSwitchTick=290,systemTicks=330,usedTicks=40,TimerSlice=30userId=0,threadId=0,prio=6,loop:4,lastSwitchTick=290,systemTicks=340,usedTicks=50,TimerSlice=30No threads ready or runnable, and no pending interrupts.Assuming the program completed.Machine halting!Ticks: total 350, idle 0, system 350, user 0Disk I/O: reads 0, writes 0Console I/O: reads 0, writes 0Paging: faults 0Network I/O: packets received 0, sent 0Cleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# Challenge 线程调度算法扩展（至少实现一种算法）可实现“时间片轮转算法”、“多级队列反馈调度算法”，或将Linux或Windows采用的调度算法应用到Nachos上。 思路：nachos启动时在system.cc中会new一个timer类，每隔一个TimerTicks大小触发时钟中断，从而让时钟向前走，时间片的大下定义在stats.h中。同时在stats.h中定义一个时间片大小变量TimerSlice，每个线程运行时间只要大于等于TimerSlice，立即放弃CPU。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135stats.h：……// nachos执行每条用户指令的时间为1Tick#define UserTick 1// 系统态无法进行指令计算，// 所以nachos系统态的一次中断调用或其他需要进行时间计算的单位设置为10Tick#define SystemTick 10// 磁头寻找超过一个扇区的时间#define RotationTime 500// 磁头寻找超过一个磁道的时间#define SeekTime 500#define ConsoleTime 100 // time to read or write one character#define NetworkTime 100 // time to send or receive one packet// 时钟中断间隔#define TimerTicks 5 // (average) time between timer interrupts// 时间片轮转算法一个时间片大小#define TimerSlice 10 ……threadtest.cc:void SimpleThread(int which)&#123; for (int num = 0; num &lt; 5; num++) &#123; int ticks = stats-&gt;systemTicks - scheduler-&gt;getLastSwitchTick(); printf(\"userId=%d,threadId=%d,prio=%d,loop:%d,lastSwitchTick=%d,systemTicks=%d,usedTicks=%d,TimerSlice=%d\\n\",currentThread-&gt;getUserId(),currentThread-&gt;getThreadId(),currentThread-&gt;getPriority(),num,scheduler-&gt;getLastSwitchTick(),stats-&gt;systemTicks,ticks,TimerSlice); // 时间片轮转算法，判断时间片是否用完 // 如果用完主动让出cpu，针对nachos内核线程算法 if(ticks &gt;= TimerSlice)&#123; printf(\"threadId=%d Yield\\n\",currentThread-&gt;getThreadId()); currentThread-&gt;Yield(); &#125; // 非抢占模式下，多个线程同时执行该接口的话，会交替执行，交替让出cpu // currentThread-&gt;Yield(); &#125;&#125;threadtest.cc:// 创建四个线程，加上主线程共五个，时间片轮转调度策略，不可抢占void ThreadPriorityTest()&#123; Thread* t1 = new Thread(\"forkThread1\", 1); printf(\"--&gt;name=%s,threadId=%d\\n\",t1-&gt;getName(),t1-&gt;getThreadId()); t1-&gt;Fork(SimpleThread, (void*)1); Thread* t2 = new Thread(\"forkThread2\", 2); printf(\"--&gt;name=%s,threadId=%d\\n\",t2-&gt;getName(),t2-&gt;getThreadId()); t2-&gt;Fork(SimpleThread, (void*)2); Thread* t3 = new Thread(\"forkThread3\", 3); printf(\"--&gt;name=%s,threadId=%d\\n\",t3-&gt;getName(),t3-&gt;getThreadId()); t3-&gt;Fork(SimpleThread, (void*)3); Thread* t4 = new Thread(\"forkThread4\", 4); printf(\"--&gt;name=%s,threadId=%d\\n\",t4-&gt;getName(),t4-&gt;getThreadId()); t4-&gt;Fork(SimpleThread, (void*)4); currentThread-&gt;Yield(); SimpleThread(0);&#125;// 运行结果，可看到usedTicks &gt;= TimserSlice时都让出cpu// 并且线程执行顺序为1 2 3 4 0，直到结束root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# ./nachos -q 3--&gt;name=forkThread1,threadId=1--&gt;name=forkThread2,threadId=2--&gt;name=forkThread3,threadId=3--&gt;name=forkThread4,threadId=4userId=0,threadId=1,prio=1,loop:0,lastSwitchTick=50,systemTicks=60,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:0,lastSwitchTick=60,systemTicks=70,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:0,lastSwitchTick=70,systemTicks=80,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:0,lastSwitchTick=80,systemTicks=90,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:0,lastSwitchTick=90,systemTicks=100,usedTicks=10,TimerSlice=10threadId=0 YielduserId=0,threadId=1,prio=1,loop:1,lastSwitchTick=100,systemTicks=110,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:1,lastSwitchTick=110,systemTicks=120,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:1,lastSwitchTick=120,systemTicks=130,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:1,lastSwitchTick=130,systemTicks=140,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:1,lastSwitchTick=140,systemTicks=150,usedTicks=10,TimerSlice=10threadId=0 YielduserId=0,threadId=1,prio=1,loop:2,lastSwitchTick=150,systemTicks=160,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:2,lastSwitchTick=160,systemTicks=170,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:2,lastSwitchTick=170,systemTicks=180,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:2,lastSwitchTick=180,systemTicks=190,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:2,lastSwitchTick=190,systemTicks=200,usedTicks=10,TimerSlice=10threadId=0 YielduserId=0,threadId=1,prio=1,loop:3,lastSwitchTick=200,systemTicks=210,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:3,lastSwitchTick=210,systemTicks=220,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:3,lastSwitchTick=220,systemTicks=230,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:3,lastSwitchTick=230,systemTicks=240,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:3,lastSwitchTick=240,systemTicks=250,usedTicks=10,TimerSlice=10threadId=0 YielduserId=0,threadId=1,prio=1,loop:4,lastSwitchTick=250,systemTicks=260,usedTicks=10,TimerSlice=10threadId=1 YielduserId=0,threadId=2,prio=2,loop:4,lastSwitchTick=260,systemTicks=270,usedTicks=10,TimerSlice=10threadId=2 YielduserId=0,threadId=3,prio=3,loop:4,lastSwitchTick=270,systemTicks=280,usedTicks=10,TimerSlice=10threadId=3 YielduserId=0,threadId=4,prio=4,loop:4,lastSwitchTick=280,systemTicks=290,usedTicks=10,TimerSlice=10threadId=4 YielduserId=0,threadId=0,prio=6,loop:4,lastSwitchTick=290,systemTicks=300,usedTicks=10,TimerSlice=10threadId=0 YieldNo threads ready or runnable, and no pending interrupts.Assuming the program completed.Machine halting!Ticks: total 350, idle 0, system 350, user 0Disk I/O: reads 0, writes 0Console I/O: reads 0, writes 0Paging: faults 0Network I/O: packets received 0, sent 0Cleaning up...root@yangyu-ubuntu-32:/mnt/nachos-3.4-Lab/nachos-3.4/threads# 内容三：遇到的困难以及解决方法困难1切换线程过程中，产生段错误，通过定位，误把销毁的线程挂入就绪对了所致。 内容四：收获及感想自己动手实现后，发现时间片轮转算法，线程调度，FIFO，时钟中断等其实并不陌生。一切只要你不懒和肯付出实际行动的难题都是纸老虎。 内容五：对课程的意见和建议暂无。 内容六：参考文献暂无。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://icoty.github.io/categories/操作系统/"},{"name":"线程调度算法","slug":"线程调度算法","permalink":"https://icoty.github.io/categories/线程调度算法/"}],"tags":[{"name":"Nachos-3.4","slug":"Nachos-3-4","permalink":"https://icoty.github.io/tags/Nachos-3-4/"},{"name":"时间片轮转调度策略","slug":"时间片轮转调度策略","permalink":"https://icoty.github.io/tags/时间片轮转调度策略/"},{"name":"FIFO线程调度策略","slug":"FIFO线程调度策略","permalink":"https://icoty.github.io/tags/FIFO线程调度策略/"},{"name":"基于优先级的可抢占式调度策略","slug":"基于优先级的可抢占式调度策略","permalink":"https://icoty.github.io/tags/基于优先级的可抢占式调度策略/"}]},{"title":"Nachos-Lab1-完善线程机制","slug":"nachos-3-4-Lab1","date":"2019-05-13T05:20:11.000Z","updated":"2019-05-23T00:03:55.000Z","comments":true,"path":"2019/05/13/nachos-3-4-Lab1/","link":"","permalink":"https://icoty.github.io/2019/05/13/nachos-3-4-Lab1/","excerpt":"","text":"Nachos是什么Nachos (Not Another Completely Heuristic Operating System)，是一个教学用操作系统，提供了操作系统框架： 线程 中断 虚拟内存（位图管理所有物理页，虚拟地址与物理地址之间的转换等） 同步与互斥机制（锁、条件变量、信号量），读者写者问题，生产者消费者问题，BARRIER问题等 线程调度（基于优先级可抢占式调度，时间片轮转算法，FIFO调度） 文件系统 系统调用 机器指令、汇编指令、寄存器……Nachos模拟了一个MIPS模拟器，运行用户程序。 目录结构12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576.├── COPYRIGHT├── gnu-decstation-ultrix // 交叉编译工具链├── nachos-3.4.zip // 未经任何修改的源码和交叉编译工具，实验就是修改源码完善各个模块的功能├── README└── nachos-3.4 // 实验过程中完善的代码 ├── test // 该目录下编写用户自己的程序，需要修改Makfile添加自己的文件 ├── bin // 用户自己的程序需要利用coff2noff转换，才能在nachos下跑起来 ├── filesys // 文件系统管理 │ ├── directory.cc // 目录文件，由目录项组成，目录项里记录了文件头所在扇区号 │ ├── directory.h │ ├── filehdr.cc │ ├── filehdr.h // 文件头数据结构，通过索引记录了文件内容实际存储的所有扇区号 │ ├── filesys.cc // 文件系统数据结构，创建/删除/读/写/修改/重命名/打开/关别等接口 │ ├── filesys.h │ ├── fstest.cc │ ├── Makefile │ ├── openfile.cc // 管理所有打开的文件句柄 │ ├── openfile.h │ ├── synchdisk.cc // 同步磁盘类，加锁保证互斥和文件系统的一致性 │ ├── synchdisk.h │ └── test ├── machine // 机器硬件模拟 │ ├── console.cc // 终端 │ ├── console.h │ ├── disk.cc // 磁盘 │ ├── disk.h │ ├── interrupt.cc // 中断处理器，利用FIFO维护一个中断队列 │ ├── interrupt.h │ ├── timer.cc // 模拟硬件时钟，用于时钟中断 │ ├── timer.h │ ├── translate.cc // 用户程序空间虚拟地址和物理之间的转换类 │ └── translate.h ├── network // 网络系统管理 │ ├── Makefile │ ├── nettest.cc │ ├── post.cc │ ├── post.h │ └── README ├── threads // 内核线程管理 │ ├── list.cc // 工具模块 定义了链表结构及其操作 │ ├── list.h │ ├── main.cc // main入口，可以传入argv参数 │ ├── Makefile │ ├── scheduler.cc // 调度器，维护一个就绪的线程队列，时间片轮转/FIFO/优先级抢占 │ ├── scheduler.h │ ├── stdarg.h │ ├── switch.c // 线程启动和调度模块 │ ├── switch.h │ ├── switch-old.s │ ├── switch.s // 线程切换 │ ├── synch.cc // 同步与互斥，锁/信号量/条件变量 │ ├── synch.dis │ ├── synch.h │ ├── synchlist.cc // 类似于一个消息队列 │ ├── synchlist.h │ ├── system.cc // 主控模块 │ ├── system.h │ ├── thread.cc // 线程数据结构 │ ├── thread.h │ ├── threadtest.cc │ ├── utility.cc │ └── utility.h ├── userprog // 用户进程管理 │ ├── addrspace.cc // 为noff文件的代码段/数据段分配空间，虚拟地址空间 │ ├── addrspace.h │ ├── bitmap.cc // 位图，用于管理扇区的分配和物理地址的分配 │ ├── bitmap.h │ ├── exception.cc // 异常处理 │ ├── Makefile │ ├── progtest.cc // 测试nachos是否可执行用户程序 │ └── syscall.h // 系统调用 └── vm // 虚拟内存管理 └── Makefile // 多线程编译: make -j4 └── Makefile.common // 各个模块公共的Makefile内容存放到这里面 └── Makefile.dep // 依赖 环境选择Linux或Unix系统，安装32位GCC开发环境，安装32的ubuntu。 源码获取https://github.com/icoty/nachos-3.4-Lab 内容一：总体概述本次Lab针对的内容是实现线程机制最基本的数据结构——进程控制块（PCB）。当一个进程创建时必然会生成一个相应的进程控制块，记录一些该线程特征，如进程ID、进程状态、进程优先级，进程开始运行时间，在cpu上已经运行了多少时间，程序计数器，SP指针，根目录和当前目录指针，文件描述符表，用户ID，组ID，指向代码段、数据段和栈段的指针等（当然，Nachos简化了进程控制块的内容）。实验的主要内容是修改和扩充PCB，主要难点在于发现修改PCB影响到的文件并进行修改。PCB是系统感知进程存在的唯一标志，且进程与PCB一一对应。可将PCB内部信息划分为：进程描述信息，进程控制信息，进程占有的资源和使用情况，进程的cpu现场。扩展字段如下： 内容二：任务完成情况任务完成列表（Y/N） Exercise1 Exercise2 Exercise3 Exercise4 第一部分 Y Y Y Y 具体Exercise的完成情况Exercise1 调研调研Linux或Windows中进程控制块（PCB）的基本实现方式，理解与Nachos的异同。 linux-4.19.23调研：Linux中的每一个进程由一个task_struct数据结构来描述。task_struct也就是PCB的数据结构。task_struct容纳了一个进程的所有信息，linux内核代码中的task_struct在linux-4.19.23/include/linux/sched.h内。 Linux内核进程状态：如下可分为运行态，可中断和不可中断态，暂停态，终止态，僵死状态，挂起状态等。 Linux内核进程调度：sched_info数据结构，包括被调度次数，等待时间，最后一次调度时间。vi linux-4.19.23/include/linux/sched.h： 123456789101112131415161718192021222324252627282930313233343536373839404142……/* Used in tsk-&gt;state: */#define TASK_RUNNING 0x0000 // 运行态#define TASK_INTERRUPTIBLE 0x0001 // 可中断#define TASK_UNINTERRUPTIBLE 0x0002 // 不可中断#define __TASK_STOPPED 0x0004 #define __TASK_TRACED 0x0008/* Used in tsk-&gt;exit_state: */#define EXIT_DEAD 0x0010#define EXIT_ZOMBIE 0x0020 // 僵死态#define EXIT_TRACE (EXIT_ZOMBIE | EXIT_DEAD)/* Used in tsk-&gt;state again: */#define TASK_PARKED 0x0040#define TASK_DEAD 0x0080#define TASK_WAKEKILL 0x0100#define TASK_WAKING 0x0200#define TASK_NOLOAD 0x0400#define TASK_NEW 0x0800#define TASK_STATE_MAX 0x1000…………struct sched_info &#123;#ifdef CONFIG_SCHED_INFO /* Cumulative counters: */ /* # of times we have run on this CPU: */ unsigned long pcount; /* Time spent waiting on a runqueue: */ unsigned long long run_delay; /* Timestamps: */ /* When did we last run on a CPU? */ unsigned long long last_arrival; /* When were we last queued to run? */ unsigned long long last_queued;#endif /* CONFIG_SCHED_INFO */&#125;;…… 时钟与锁：内核需要记录进程在其生存期内使用CPU的时间以便于统计、计费等有关操作。进程耗费CPU的时间由两部分组成：一是在用户态下耗费的时间，一是在系统态下耗费的时间。这类信息还包括进程剩余的时间片和定时器信息等，以控制相应事件的触发。 文件系统信息：进程可以打开或关闭文件，文件属于系统资源，Linux内核要对进程使用文件的情况进行记录。 虚拟内存信息：除了内核线程，每个进程都拥有自己的地址空间，Linux内核中用mm_struct结构来描述。 物理页管理信息：当物理内存不足时，Linux内存管理子系统需要把内存中部分页面交换到外存，并将产生PageFault的地址所在的页面调入内存，交换以页为单位。这部分结构记录了交换所用到的信息。 多处理器信息：与多处理器相关的几个域，每个处理器都维护了自己的一个进程调度队列，Linux内核中没有线程的概念，统一视为进程。 处理器上下文信息：当进程因等待某种资源而被挂起或停止运行时，处理机的状态必须保存在进程的task_struct，目的就是保存进程的当前上下文。当进程被调度重新运行时再从进程的task_struct中把上下文信息读入CPU（实际是恢复这些寄存器和堆栈的值），然后开始执行。 与Nachos的异同：Nachos相对于Linux系统的线程部分来讲，要简单许多。它的PCB仅有几个必须的变量，并且定义了一些最基本的对线程操作的函数。Nachos线程的总数目没有限制，线程的调度比较简单，而且没有实现线程的父子关系。很多地方需要完善。 Exercise2 源代码阅读code/threads/main.cc：main.cc是整个nachos操作系统启动的入口，通过它可以直接调用操作系统的方法。通过程序中的main函数，配以不同的参数，可以调用Nachos操作系统不同部分的各个方法。 code/threads/threadtest.cc：nachos内核线程测试部分，Fork两个线程，交替调用Yield()主动放弃CPU，执行循环体，会发现线程0和线程1刚好是交替执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566int main(int argc, char **argv)&#123; int argCount; // the number of arguments DEBUG('t', \"Entering main\"); (void) Initialize(argc, argv);#ifdef THREADS for (argc--, argv++; argc &gt; 0; argc -= argCount, argv += argCount) &#123; argCount = 1; switch (argv[0][1]) &#123; case 'q': testnum = atoi(argv[1]); argCount++; break; case 'T': if(argv[0][2] == 'S') testnum = 3; break; default: testnum = 1; break; &#125; &#125; ThreadTest();#endif ……&#125;threadtest.cc// 线程主动让出cpu,在FIFO调度策略下能够看到多个线程按顺序运行void SimpleThread(int which)&#123; for (int num = 0; num &lt; 5; num++) &#123; int ticks = stats-&gt;systemTicks - scheduler-&gt;getLastSwitchTick(); // 针对nachos内核线程的时间片轮转算法，判断时间片是否用完，如果用完主动让出cpu if(ticks &gt;= TimerSlice)&#123; currentThread-&gt;Yield(); &#125; // 多个线程同时执行该接口的话，会交替执行，交替让出cpu // currentThread-&gt;Yield(); &#125;&#125;root@yangyu-ubuntu-32:/mnt/nachos-3.4/code/threads# root@yangyu-ubuntu-32:/mnt/nachos-3.4/code/threads# ./nachos -q 1userId=0,threadId=0,prio=5,loop:0,lastSwitchTick=0,systemTicks=20,usedTicks=20,TimerSlice=30userId=0,threadId=1,prio=5,loop:0,lastSwitchTick=20,systemTicks=30,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=5,loop:1,lastSwitchTick=30,systemTicks=40,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=5,loop:1,lastSwitchTick=40,systemTicks=50,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=5,loop:2,lastSwitchTick=50,systemTicks=60,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=5,loop:2,lastSwitchTick=60,systemTicks=70,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=5,loop:3,lastSwitchTick=70,systemTicks=80,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=5,loop:3,lastSwitchTick=80,systemTicks=90,usedTicks=10,TimerSlice=30userId=0,threadId=0,prio=5,loop:4,lastSwitchTick=90,systemTicks=100,usedTicks=10,TimerSlice=30userId=0,threadId=1,prio=5,loop:4,lastSwitchTick=100,systemTicks=110,usedTicks=10,TimerSlice=30No threads ready or runnable, and no pending interrupts.Assuming the program completed.Machine halting!Ticks: total 130, idle 0, system 130, user 0Disk I/O: reads 0, writes 0Console I/O: reads 0, writes 0Paging: faults 0Network I/O: packets received 0, sent 0Cleaning up... code/threads/thread.h：这部分定义了管理Thread的数据结构，即Nachos中线程的上下文环境。主要包括当前线程栈顶指针，所有寄存器的状态，栈底，线程状态，线程名。当前栈指针和机器状态的定义必须必须放作为线程成员变量的前两个，因为Nachos执行线程切换时，会按照这个顺序找到线程的起始位置，然后操作线程上下文内存和寄存器。在Thread类中还声明了一些基本的方法，如Fork()、Yield()、Sleep()等等，由于这些方法的作用根据名字已经显而易见了，在此不再赘述。 code/threads/thread.cc： Thread.cc中主要是管理Thread的一些事务。主要接口如下： Fork(VoidFunctionPtr func,int arg)：func是新线程运行的函数，arg是func函数的入参，Fork的实现包括分为几步：分配一个堆栈，初始化堆栈，将线程放入就绪队列。 Finish()：不是直接收回线程的数据结构和堆栈，因为当前仍在这个堆栈上运行这个线程。先将threadToBeDestroyed的值设为当前线程，在Scheduler的Run()内切换到新的线程时在销毁threadToBeDestroyed。Yield()、Sleep()。这里实现的方法大多是都是原子操作，在方法的一开始保存中断层次关闭中断，并在最后恢复原状态。 Yield()：当前线程放入就绪队列，从scheduler就绪队列中的找到下一个线程上cpu，以达到放弃CPU的效果。 Exercise3 扩展线程的数据结构Exercise4 增加全局线程管理机制这里我把Exercise3和Exercise4放在一起完成。 在Thread类中添加私有成员userId和threadId，添加公有接口getUserId()和getThreadId()，userId直接沿用Linux个getuid()接口。 system.h内部添加全局变量maxThreadsCount=128，全局数组threads[maxThreadsCount]，每创建一个线程判断并分配threadId。 -TS模仿Linux的PS命令打印所有线程信息，仔细阅读list.cc代码和scheduler.cc的代码，就会发现可以直接用scheduler.cc::Print()接口，不用我们重新造轮子。 在system.cc中的void Initialize(int argc, char argv)函数体对全局数组初始化。如下我用root用户执行分配的userId为0，切换到其他用户userId会发生变化，线程id分别为0和1。当线程数超过128个线程时，ASSERT断言报错。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455threadtest.cc：void ThreadTest()&#123; switch (testnum) &#123; case 1: ThreadTest1(); break; case 2: ThreadCountLimitTest(); break; case 3: ThreadPriorityTest(); break; case 4: ThreadProducerConsumerTest(); break; case 5: ThreadProducerConsumerTest1(); break; case 6: barrierThreadTest(); break; case 7: readWriteThreadTest(); break; default: printf(\"No test specified.\\n\"); break; &#125;&#125;// 线程最多128个，超过128个终止运行void ThreadCountLimitTest()&#123; for (int i = 0; i &lt;= maxThreadsCount; ++i) &#123; Thread* t = new Thread(\"fork thread\"); printf(\"thread name = %s, userId = %d, threadId = %d\\n\", t-&gt;getName(), t-&gt;getUserId(), t-&gt;getThreadId()); &#125;&#125;root@yangyu-ubuntu-32:/mnt/nachos-3.4/code/threads# ./nachos -TSthread name = fork thread, userId = 0, threadId = 1thread name = fork thread, userId = 0, threadId = 2thread name = fork thread, userId = 0, threadId = 3……thread name = fork thread, userId = 0, threadId = 122thread name = fork thread, userId = 0, threadId = 123thread name = fork thread, userId = 0, threadId = 124thread name = fork thread, userId = 0, threadId = 125thread name = fork thread, userId = 0, threadId = 126thread name = fork thread, userId = 0, threadId = 127allocatedThreadID fail, maxThreadsCount:[128]Assertion failed: line 73, file \"../threads/thread.cc\"Aborted (core dumped)root@yangyu-ubuntu-32:/mnt/nachos-3.4/code/threads# 内容三：遇到的困难以及解决方法困难1开始make编译出错，通过定位到具体行，复制出来手动执行，发现是gcc交叉编译工具链路径不对。 困难2刚开始修改代码验证效果，重定义错误，外部文件全局变量使用方式不对导致。 内容四：收获及感想动手实践很重要，不管你是做什么事、什么项目、什么作业，一定要落实到代码和跑到程序上面来。绝知此事要躬行，学习来不得半点虚假。 内容五：对课程的意见和建议暂无。 内容六：参考文献暂无。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://icoty.github.io/categories/操作系统/"}],"tags":[{"name":"PCB","slug":"PCB","permalink":"https://icoty.github.io/tags/PCB/"},{"name":"Nachos-3.4","slug":"Nachos-3-4","permalink":"https://icoty.github.io/tags/Nachos-3-4/"}]},{"title":"另类P、V操作问题-详细图解","slug":"stack-pv","date":"2019-04-25T07:06:35.000Z","updated":"2019-05-31T15:15:36.000Z","comments":true,"path":"2019/04/25/stack-pv/","link":"","permalink":"https://icoty.github.io/2019/04/25/stack-pv/","excerpt":"","text":"问题模型有一个系统，定义如下P、V操作：123456789P(s)： s.count--; if s&lt;0 then 将本进程插入相应队列末尾等待; V(s): s.count++; if s&lt;=0 then 从相应等待队列队尾唤醒一个进程，将其插入就绪队列; 思考并回答:a. 这样定义P、V操作是否有问题?b. 试用这样的P、V操作实现N个进程竞争使用某一共享变量的互斥机制。c. 对于b的解法，有无效率更高的方法。如有，试问降低了多少复杂性? 分析a. 当然有问题，假设s=2，现有进程p1、p2按顺序来请求共享资源A，p1和p2直接获取A，假设p1和p2都还未释放A的时候，p3、p4、p5按顺序也来请求A，这时s的等待队列L为：(尾)p5p4p3(头)，然后p1释放A，执行V(s)操作从L队尾唤醒p5，L变为：(尾)p4p3(头)。这时A被p2和p5持有，且p2和p5都未释放A的时候，假设这时p1又来请求A，p1被挂起，L变为：(尾)p1p4p3(头)。然后p2释放A执行V(s)操作从L队尾唤醒p1，你会发现p1又竞争到了A，而p3和p4还一次都未竞争到，这会导致越靠近L队首的p3和p4越容易饿死，出现饥饿现象。问题的根源就在于这样定义的P、V操作，由于在信号量的等待队列上是先进后出导致的，这属于栈P、V。 b. 解决方案这里以N个进程为例进行一般化分析，定义信号量数组S[N-1]，共有N-1个信号量，下标从0～N-2，其中S[i] = N-i-1，表示第i+1个信号量S[i]的初值为N-i-1，初值为何取这个看后面分析，下为伪码。 123456789Semaphore S[N-1]; // S[i] = N-i-1void func()&#123; for(int i=0 ; i&lt;n-1 ; i++) P(S[i]); // 临界区 Critical Section for(int i=n-2 ; i&gt;=0 ; i--) V(S[i]);&#125; 一定要注意P(S[i])操作中的i是从0～N-2，而V(S[i])的i是反过来的从N-2～0，这个很重要，这个就是多级队列的精髓，顺序不能换。下面的分析，假设t1时刻p1进入临界区还没出来之前，t2～tN时刻p2～pN按顺序来请求进入临界区，那么p2～pN都执行for循环，分别被挂起在信号量N-2～0的等待队列上，并且每个信号量的等待队列上有且只有一个进程被挂起。在tN+1时刻p1出临界区，由于V(S[i])是从N-2～0，因此等待在LN-2上的P2最先被唤醒，然后L2进入临界区。之后按顺序p3～pN依次被唤醒并依次挂入就绪队列等待被调度，而处理器从就绪队列进行调度是FIFO，与请求临界区的顺序一致，饥饿现象得以解决。 该方法的资源复杂度为O(N-1)，需要N-1个信号量。 c. 优化方法除了前面的办法，已经可以确定存在更优方案能把资源复杂度降为O(logN)。","categories":[{"name":"同步机制","slug":"同步机制","permalink":"https://icoty.github.io/categories/同步机制/"},{"name":"互斥机制","slug":"互斥机制","permalink":"https://icoty.github.io/categories/互斥机制/"}],"tags":[{"name":"信号量","slug":"信号量","permalink":"https://icoty.github.io/tags/信号量/"},{"name":"临界区","slug":"临界区","permalink":"https://icoty.github.io/tags/临界区/"},{"name":"P/V操作","slug":"P-V操作","permalink":"https://icoty.github.io/tags/P-V操作/"}]},{"title":"Docker最简教程","slug":"docker","date":"2019-04-22T12:49:16.000Z","updated":"2019-06-07T02:59:16.000Z","comments":true,"path":"2019/04/22/docker/","link":"","permalink":"https://icoty.github.io/2019/04/22/docker/","excerpt":"","text":"本文旨在让你用最短的时间弄懂Docker命令操作，什么虚拟化都太泛泛了，不讲大道理，实践出真知，让你从此的日常开发和工作中在面对Docker时不再茫然失措而是得心应手。本文也不谈安装，我实在认为作为程序员，要是我在这里教你如何安装Docker，既是在浪费你的时间也是在浪费我的时间，请参考Docker安装； Docker Hub是Docker官方维护的一个公共仓库，其中已经包括了数量超过15 000 的镜像，开发者可以注册自己的账号，并自定义自己的镜像进行存储，需要的时候可以直接拿来用，同时也能够分享，有点类似于Github，如想注册可移步 Docker Hub，注册与否不影响接下来的操作。 实践出真知我认为只要你不是专门研究这个的，那么你只需学会如何使用Docker的一些基本命令，使自己的日常开发和工作不受阻碍，弄清Docker和容器之间的区别，为什么现在很多企业流行Docker，这个东西解决了啥问题，有啥优势就够了。关于Docker是什么有何优势，这里挑了一个简洁的博客链接。 接下来从零开始，首先从docker hub仓库上拉取centos镜像，带你走进docker日常命令，学会这些命令，足以应对你的日常开发和工作中关于docker的问题。 search: 从docker hub仓库搜索带centos的所有镜像。1$docker seach centos images: 查看本地所有镜像，pull前左侧REPOSITORY栏无centos。REPOSITORY表示镜像被归入到本地的仓库，比如icoty1/lamp:v1.0.0表示本地icoty1仓库下有一个镜像名为lamp,其TAG(版本)为v1.0.0，每个镜像有一个IMAGE ID唯一标识该镜像，SIZE为镜像大小。1$docker images pull: 从docker hub远程仓库把centos镜像拉到本地，pull后再次执行images，会发现centos已经被拉到本地。1$docker pull centos ps: 只列出正在运行的容器。1$docker ps ps -a: 列出所有容器, 每一个容器有一个CONTAINER ID唯一标识该容器；IMAGE表示该容器是基于哪个镜像生成的，COMMAND是容器启动时执行的命令，可以传入也可以不传入；STATUS是容器当前的状态，Exit是已停止，Up是正在运行。1$docker ps -a run: 从镜像衍生一个新的容器并运行；-d后台模式运行容器，-i交互模式运行容器；-p把主机80关口映射到容器的80端口，因为容器具有封闭性，容器外部不能直接访问容器内部的端口，通过映射后，主机80端口收到数据后会转发到容器内部的80端口，不过在容器内是可以直接访问容器外的主机的；-v把主机的/Users/yangyu/ide/LeetCode/目录映射到容器的/LeetCode/，容器内若无/LeetCode/目录会自动创建，用于实现主机和容器之间的目录共享，在两个目录下操作文件是对等的；centos:latest是镜像名称，可以换成IMAGE ID，二者等价；/bin/bash是容器启动时执行的命令，还可以带参数，这个不懂的可以搜索下。执行docker run后再次执行ps命令，能够看到运行中的容器多了一个。1$docker run -d -ti -p 80:80 -v /Users/yangyu/ide/LeetCode/:/LeetCode/ centos:latest /bin/bash cp: 拷贝主机/Users/yangyu/ide/LeetCode/Database/目录到容器eaf43b370eb7根目录。1$docker cp /Users/yangyu/ide/LeetCode/Database/ eaf43b370eb7:/ exec: 执行该命令进入容器eaf43b370eb7内，进入容器后在容器内/LeetCode/目录下新建readme.py2，在主机/Users/yangyu/ide/LeetCode/目录下能够看到该文件。12$docker exec -it eaf43b370eb7 /bin/bash$cp /LeetCode/readme.py /LeetCode/readme.py2 cp: 从容器eaf43b370eb7根目录下拷贝目录/Database到主机的/Users/yangyu/ide/LeetCode/Database/目录下。1$docker cp eaf43b370eb7:/Database/ /Users/yangyu/ide/LeetCode/Database/ stop/restart: 停止容器eaf43b370eb7然后查看其状态变为Exited；然后通过restart命令重启，容器又处于运行态。12$docker stop eaf43b370eb7$docker restart eaf43b370eb7 tag: 把centos镜像归入icoty1仓库下名称为centos，TAG为7，TAG随你定。1$docker tag centos icoty1/centos:7 commit: 最初pull下来的centos镜像是最简版本，里面没有安装mysql vim等；最初pull下来后基于其运行一个容器，你在容器内部可以安装你需要的环境，比如mysql，apache，nginx，hexo博客等，安装好后通过commit命令把容器提交为一个新的镜像，以后凡是从这个新的镜像运行的容器都带有你安装的内容。-m是提交说明；-p是执行commit命令时暂停该容器；eaf43b370eb7是容器ID。1$docker commit -m \"提交我的自定义镜像，centos7内安装mysql，版本号v1.0.0\" -p eaf43b370eb7 icoty1/centos7-mysql:v1.0.0 save: 把镜像03cb190015bf打包成主机目录/Users/yangyu/ide/LeetCode/下的centos7-mysql.tar，然后你可以通过U盘拷贝到其他机器上，在其他机器上通过load命令可以把centos7-mysql.tar加载成一个镜像。1$docker save 03cb190015bf &gt; /Users/yangyu/ide/LeetCode/centos7-mysql.tar load: 把centos7-mysql.tar加载为镜像，因为这个包是从我主机上的镜像03cb190015bf打出来的，所以执行load的时候直接返回镜像03cb190015bf，如果在其他机器上执行会生成一个新的镜像ID。1$docker load &lt; /Users/yangyu/ide/LeetCode/centos7-mysql.tar push: 把本地icoty1仓库下TAG为v1.0.0的镜像icoty1/centos7-mysql推到远程仓库docker hub上的icoty1仓库下保存，执行push前需要在本地icoty1已经登陆。push成功之后，其他人就可以通过pull命令拉取你的镜像使用了，相当于git clone操作。12$docker push icoty1/centos7-mysql:v1.0.0$docker pull icoty1/centos7-mysql:v1.0.0 rm: 删除容器eaf43b370eb7，运行中的容器无法删除。1$docker rm eaf43b370eb7 rmi: 删除镜像03cb190015bf，在这之前必须删除由该镜像衍生出来的所有容器删除，否则会删除失败，执行该命令后通过images发现镜像已经没有了。1$docker rmi 03cb190015bf build: 如下以我搭建hexo博客的Dockerfile举例说明。12345678910111213141516171819202122232425# 基础镜像，icoty1/ubuntu-hexo-blog:latest在本地仓库必须已经存在FROM icoty1/ubuntu-hexo-blog:latest# 维护人员信息，可写可不写MAINTAINER icoty1 \"https://icoty.github.io\" # 暴露容器的4000端口，这样主机就可以映射端口到4000了EXPOSE 4000/*自动安装所需环境，可替换成安装mysql vim等你需要的命令 *hexo部分插件安装，使支持rss，图片，字数统计等功能 */RUN npm install -g hexo-cli \\&amp;&amp; npm install hexo-server --save \\&amp;&amp; hexo init blog &amp;&amp; cd /blog \\&amp;&amp; npm install \\&amp;&amp; npm install hexo-deployer-git --save \\&amp;&amp; npm install hexo-migrator-rss --save \\ &amp;&amp; npm install hexo-asset-image --save \\&amp;&amp; npm install hexo-wordcount --save \\&amp;&amp; npm install hexo-generator-sitemap --save \\ &amp;&amp; npm install hexo-generator-baidu-sitemap --save \\ &amp;&amp; npm install hexo-helper-live2d --save \\&amp;&amp; git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia \\&amp;&amp; sed \"s/theme: landscape/theme: yilia/g\" -i /blog/_config.yml 12$ docker build -t icoty1/ubuntu-hexo . # icoty1/ubuntu-hexo是新的镜像的名字$ docker images # build后会多出icoty1/ubuntu-hexo镜像 镜像与容器为了便于理解，你可以把镜像理解成一个初始模版A，通过这个模板A你可以复制出模板B、模板C等，模板B和模板C在这里就相当于容器，突然某一天你发现模板A现有的内容已经不能满足你的需求了（比如模板A没有安装Mysql，而你需要安装Mysql），这时你就只能自定义新的模板(相当于自定义新的符合你的要求的镜像)，而自定义方式则可以从模板B或模板C中安装Mysql，安装成功之后，通过docker commit命令将模板B或模板C提交成一个新的初始模板A1（也就是新的镜像），以后所有从模板A1运行的容器就都有Mysql了，然后你就有模板A和模板A1了（就是两个镜像）。 建议实际操作部分，对各个命令有疑问的，相信我，直接执行一遍才是解决你心中疑虑的不二之法，如果你的命令参数不正确，顶多就是报错和执行不成功，不会让你的主机崩溃，最坏也就不过重新执行一遍，IT这个职业，其本身就是一个不断试错、犯错和总结经验的过程，如果你学到了，请我喝奶茶吧，小生会一直奋斗在原创的路上。 参考文献Docker命令Docker中文社区","categories":[{"name":"Docker","slug":"Docker","permalink":"https://icoty.github.io/categories/Docker/"}],"tags":[{"name":"Dockerfile","slug":"Dockerfile","permalink":"https://icoty.github.io/tags/Dockerfile/"},{"name":"镜像","slug":"镜像","permalink":"https://icoty.github.io/tags/镜像/"},{"name":"容器","slug":"容器","permalink":"https://icoty.github.io/tags/容器/"},{"name":"Docker教程","slug":"Docker教程","permalink":"https://icoty.github.io/tags/Docker教程/"}]},{"title":"Hexo+Github博客最简教程-Dockerfile自动搭建","slug":"docker-hexo-blog","date":"2019-04-18T15:23:05.000Z","updated":"2019-06-07T03:00:25.000Z","comments":true,"path":"2019/04/18/docker-hexo-blog/","link":"","permalink":"https://icoty.github.io/2019/04/18/docker-hexo-blog/","excerpt":"","text":"闲谈拿出你的气质，打开你的电脑，借你半小时搭建好属于你的hexo博客，小生用dockerfile自动帮你搭建好；你只需要在你的mac或linux或windows上提前把docker安装好，如何安装不是我的重点，请参考Docker安装；作为程序员，博客就像你的影子，我都已经忘了内心曾经多少次告诫自己，一定要搭建一个属于自己的技术博客，奈何日复一日过去了，近来终于落实到行动上来，所谓明日复明日，明日何其多，早晚要做的事，劝君晚做不如早做。 搭建Hexo获取基础镜像Docker安装成功之后方能进行接下来的操作，如果对Docker基本命令不熟悉又想真懂的可以看下我的另一篇文章Docker最简教程。首先从我的Docker Hub仓库上获取基础镜像： 12$ docker pull icoty1/ubuntu-hexo-blog # 从Docker hub上的icoty1用户下获取基础镜像$ docker images # 查看本地所有镜像，会发现icoty1/ubuntu-hexo-blog已经被pull下来了 生成Dockerfile进入一个空的目录下新建空文件Dockerfile，复制如下内容： 123456789101112131415161718192021# 基础镜像FROM icoty1/ubuntu-hexo-blog:latestMAINTAINER icoty1 \"https://icoty.github.io\" EXPOSE 4000# hexo部分插件安装，使支持rss，图片，字数统计等功能RUN npm install -g hexo-cli \\&amp;&amp; npm install hexo-server --save \\&amp;&amp; hexo init blog &amp;&amp; cd /blog \\&amp;&amp; npm install \\&amp;&amp; npm install hexo-deployer-git --save \\&amp;&amp; npm install hexo-migrator-rss --save \\&amp;&amp; npm install hexo-asset-image --save \\&amp;&amp; npm install hexo-wordcount --save \\&amp;&amp; npm install hexo-generator-sitemap --save \\&amp;&amp; npm install hexo-generator-baidu-sitemap --save \\&amp;&amp; npm install hexo-helper-live2d --save \\&amp;&amp; git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia \\&amp;&amp; sed \"s/theme: landscape/theme: yilia/g\" -i /blog/_config.yml 更换主题Dockerfile中的最后两行内容表示的含义是从github上把hexo-theme-yilia克隆下来并重命名成yilia，然后放到容器的/blog/themes/目录下，其中hexo-theme-yilia是hexo的主题，hexo有很多种主题，用每一种主题搭建出来的hexo博客界面美观和布局都不尽相同，你可以通过hexo官网上浏览每一种主题长啥样子，通过github获取主题的源码仓库，选择一个你喜欢的主题，并相应的修改这两行。假如你从github选择的主题仓库地址是https://github.com/yscoder/hexo-theme-indigo.git ， 那么你需要按照如下方式进行修改，如果你就想用yilia，那么你不需要做任何修改，我用的主题是https://github.com/theme-next/hexo-theme-next.git 12&amp;&amp; git clone https://github.com/yscoder/hexo-theme-indigo.git themes/indigo \\&amp;&amp; sed \"s/theme: landscape/theme: indigo/g\" -i /blog/_config.yml 构建Hexo镜像在Dockerfile的同级目录执行： 12$ docker build -t icoty1/ubuntu-hexo . # 把icoty1/ubuntu-hexo替换成你取的名字$ docker images # 能够看到多出一条记录icoty1/ubuntu-hexo，并能看到该镜像的[IMAGE ID] 启动容器123456789/* 把[IMAGE ID]替换成上一步构建出来的镜像的ID，该句执行成功会多出来一个容器并有一个[CONTAINER ID] * -v /home/yangyu/blog/：/blog/是把本机的/home/yangyu/blog/目录映射到容器的/blog/目录 * 通过目录映射，你只需要在本机编辑/home/yangyu/blog/目录下的文件，而不用每次都进入容器/blog/目录下编辑文件 * -p 4000:4000 将主机的4000端口映射到容器的4000端口 *\\$ docker run -d -ti -p 4000:4000 -v /home/yangyu/blog/：/blog/ [IMAGE ID] /bin/bash $ docker ps -a # 执行该句列出当前所有的容器$ docker exec -it [CONTAINER ID] /bin/bash # 根据前一步的容器ID进入该容器内部$ cd /blog/ &amp;&amp; hexo s # 进入容器内部的/blog/目录下，启动hexo 浏览器测试浏览器访问http://localhost:4000 ，出现下图说明已经成功，以后你的博客配置，文章撰写和发布等，都在/home/yangyu/blog/目录下进行，这和在容器内部/blog/目录下操作完全对等。 Hexo部署到Github注册Github账户，如果已经注册，跳过此步；在github上仓库“用户名.github.io”，比如我的用户名为icoty，仓库名则为：icoty.github.io； 执行如下命令生成ssh key，执行完后复制~/.sshid_rsa.pub文件内的全部内容，按照图示添加ssh keys，并粘贴保存到Key栏中，Title栏随便取。 1234$ cd ~/.ssh$ ssh-keygen -t rsa -C \"youremail@example.com\" # 全程回车$ git config --global user.name \"你用github用户名\"$ git config --global user.email \"你的github邮箱地址\" 配置Hexo主题编辑/blog/_config.yml文件，编辑标题、描述信息、Github信息，下图参见我的： 1234567891011121314# Sitetitle: 阳光沥肩头 仿佛自由人 # 标题subtitle: # 子标题description: Linux C++服务端 # 描述信息keywords: author: icotylanguage: zh-CN # 语言timezone: # 时区deploy: - type: git repository: git@github.com:icoty/icoty.github.io.git # 设置repository对应的链接 branch: master # 设置提交到的分支 message: Site updated at &#123;&#123; now(\"YYYY-MM-DD HH:mm:ss\") &#125;&#125; # 设置我们提交的信息 执行如下命令发布到github上，通过“https://你的github用户名.github.io”访问，我的是https://icoty.github.io 12$hexo generate$hexo deploy # 部署到GitHub 编辑/blog/themes/yilia/_config.yml文件，自定义其他配置，如友链、评论、分享、头像等，这些配置并不是一定要做，做不做都行，只是配置的完善些，你的Hexo博客界面看起来美观些，如何配置在此不一一赘述，请自行查看对应主题的官方文档和Github说明。如果你能操作这里，说明我这个教程还是有效的，感谢你的坚持！ Hexo命令Hexo搭建好后，你可以写博客发布到GitHub 上，别人通过“https://你的github用户名.github.io”就能访问你的博客和看到你写的文章，而这个章节就是教你怎么在本地写你的博客，写博客用的MarkDown语法，推荐你安装MarkDown编辑器Typora。下面列出写博客过程中常用的命令，这些命令都需要走到/blog/目录下执行。 123456789$hexo new \"my-hexo\" #新建my-hexo文章，在/blog/source/_post/目录下生成my-hexo.md，在这个文件里面写你的文章$hexo generate # 文章写好后保存，然后执行这条命令，生成静态页面至public目录$hexo s # 然后开启预览访问端口（默认端口4000，'ctrl+c'关闭server，‘ctrl+z’放到后台运行），通过http://localhost:4000 查看效果，如果满意就执行下一条命令发布到github$hexo deploy # 发布到github，通过https://你用github用户名.github.io 访问$hexo clean # 有时你写文章和配置其他内容后，老是不生效，就执行下该命令清除缓存文件 (db.json) 和已生成的静态文件 (public)，不是删除，你的文章仍然在的$nohup hexo s &amp; # 启动hexo以后台方式运行$hexo new page \"About\" #新建About页面，这个是配置Hexo界面多出来一个About布局$hexo help # 查看帮助$hexo version #查看Hexo的版本 MarkDown语法这个比较基础，网上教程也一大堆，MarkDown很容易学，放心比九九表容易多了，只要你用markdown实际动手写过一篇博文后就上手了，因此没啥可说的。 Next主题配置接下来的内容是针对next主题的配置，因为我选择的是next，不同主题可能有差异，特此说明。 修改文章内链接文本样式打开themes/next/source/css/_common/components/post/post.styl文件，在文件最后且在@import之前添加如下代码：1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; //原始链接颜色 border-bottom: none; border-bottom: 1px solid #0593d3; //底部分割线颜色 &amp;:hover &#123; color: #fc6423; //鼠标经过颜色 border-bottom: none; border-bottom: 1px solid #fc6423; //底部分割线颜色 &#125;&#125; 文章末尾添加“文章结束”标记在themes/next/layout/_macro/目录下新建passage-end-tag.swig，填入如下内容：12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=\"text-align:center;color: #ccc;font-size:14px;\"&gt;-------------本文结束&lt;i class=\"fa fa-paw\"&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 然后编辑themes/next/layout/_macro/post.swig，按照下图添加代码块：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 最后编辑themes/next/_config.yml，添加如下内容：123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true 添加网页加载进度条打开themes/next/_config.yml，搜索“pace:”，设置为true。1pace: true 设置文章的显示顺序编辑node_modules/hexo-generator-index/lib/generator.js，在return之前添加如下代码：12345678910111213posts.data = posts.data.sort(function(a, b) &#123;if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排&#125;else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1;&#125;else if(!a.top &amp;&amp; b.top) &#123; return 1;&#125;else return b.date - a.date; // 都没定义按照文章日期降序排&#125;) 然后在每篇文章的头部添加top字段，top值越大的文章显示越靠前。12345678---title: Hexo+Github博客最简教程-Dockerfile自动搭建date: 2019-04-18 15:23:05top: 6tags: [Hexo, Dockerfile, Linux, Github]categories: [IDE]copyright: ture--- 添加底部的小图标打开themes/next/layout/_partials/footer.swig搜索with-love，修改为如下代码。从fontawesom选择你喜欢的图标名称，我这里选择的是heart。123&lt;span class=\"with-love\" id=\"animate\"&gt; &lt;i class=\"fa fa-heart\" aria-hidden = \"true\"&gt;&lt;/i&gt;&lt;/span&gt; 文章底部添加版权信息在themes/next/layout/_macro/下新建 my-copyright.swig，填入如下内容：123456789101112131415161718192021222324252627282930&#123;% if page.copyright %&#125;&lt;div class=\"my_post_copyright\"&gt; &lt;script src=\"//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js\"&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script src=\"https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js\"&gt;&lt;/script&gt; &lt;script src=\"https://unpkg.com/sweetalert/dist/sweetalert.min.js\"&gt;&lt;/script&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=\"&#123;&#123; url_for(page.path) &#125;&#125;\"&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=\"/\" title=\"访问 &#123;&#123; theme.author &#125;&#125; 的个人博客\"&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(\"YYYY年MM月DD日 - HH:MM\") &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format(\"YYYY年MM月DD日 - HH:MM\") &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=\"&#123;&#123; url_for(page.path) &#125;&#125;\" title=\"&#123;&#123; page.title &#125;&#125;\"&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class=\"copy-path\" title=\"点击复制文章链接\"&gt;&lt;i class=\"fa fa-clipboard\" data-clipboard-text=\"&#123;&#123; page.permalink &#125;&#125;\" aria-label=\"复制成功！\"&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=\"fa fa-creative-commons\"&gt;&lt;/i&gt; &lt;a rel=\"license\" href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\" target=\"_blank\" title=\"Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)\"&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard('.fa-clipboard'); $(\".fa-clipboard\").click(function()&#123; clipboard.on('success', function()&#123; swal(&#123; title: \"\", text: '复制成功', icon: \"success\", showConfirmButton: true &#125;); &#125;); &#125;); &lt;/script&gt;&#123;% endif %&#125; 然后在themes/next/source/css/_common/components/post/下新建my-post-copyright.styl，填入如下内容：123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 接着编辑themes/next/layout/_macro/post.swig文件，按照下图位置添加如下代码：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'my-copyright.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 接着在themes/next/source/css/_common/components/post/post.styl文件最后添加如下代码：1@import \"my-post-copyright\" 然后，还需要在文章的头部添加copyright字段：12345678---title: Hexo+Github博客最简教程-Dockerfile自动搭建date: 2019-04-18 15:23:05top: 6tags: [Hexo, Dockerfile, Linux, Github]categories: [IDE]copyright: ture--- 最后，编辑根目录下的_config.yml文件，把url换成你的主页：123456# URL## If your site is put in a subdirectoryurl: https://icoty.github.io # 这里换成你的主页root: /permalink: :year/:month/:day/:title/permalink_defaults: 添加网易云音乐外链登陆网易云音乐网页版；点击个人头像“我的主页”；然后能够看到“我创建的歌单”，如果没有则创建一个歌单；选中一个歌单点进去，能够看到“歌曲列表”，点击“歌曲列表”右边的“生成外链播放器”；然后点击右下角的“复制代码”，粘贴到themes/next/layout/_macro/sidebar.swig文件中指定的位置即可，我的是放在侧栏中”友链”下面的。 设置文章缩略显示编辑themes/next/_config.yml，搜索auto_excerpt，设置为true：123456# Automatically Excerpt (Not recommend).# 设置文章不显示全部 点进去再显示全部# Use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 150 自定义代码块样式打开themes\\next\\source\\css_custom\\custom.styl，添加如下内容：123456789101112131415// Custom styles.code &#123; color: #ff7600; background: #fbf7f8; margin: 2px;&#125;// 大代码块的自定义样式.highlight, pre &#123; margin: 5px 0; padding: 5px; border-radius: 3px;&#125;.highlight, code, pre &#123; border: 1px solid #d6d6d6;&#125; 把一篇文章归为多类如下会把该文章归为Linux/IPC类。123categories: - Linux - IPC 如下会把该文章归为Linux/IPC和TCP两类。123categories: - [Linux, ICP] - TCP 参考文献https://www.jianshu.com/p/9f0e90cc32c2https://www.jianshu.com/p/bff1b1845ac9","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://icoty.github.io/categories/Hexo/"}],"tags":[{"name":"Dockerfile","slug":"Dockerfile","permalink":"https://icoty.github.io/tags/Dockerfile/"},{"name":"Hexo命令","slug":"Hexo命令","permalink":"https://icoty.github.io/tags/Hexo命令/"},{"name":"镜像","slug":"镜像","permalink":"https://icoty.github.io/tags/镜像/"},{"name":"容器","slug":"容器","permalink":"https://icoty.github.io/tags/容器/"}]},{"title":"基于共享内存、信号、命名管道和Select模型实现聊天窗口","slug":"ipc-chat","date":"2019-04-18T00:16:06.000Z","updated":"2019-05-28T03:47:20.000Z","comments":true,"path":"2019/04/18/ipc-chat/","link":"","permalink":"https://icoty.github.io/2019/04/18/ipc-chat/","excerpt":"","text":"问题模型 A、B两个进程通过管道通信，A 进程每次接收到的数据通过共享内存传递给A1进程显示，同理，B进程每次接收到的数据通过共享内存传递给B1进程显示； 对于A、B 进程，采用ctrl+c（实际为SIGINT信号）方式退出，A、B进程通过捕捉SIGINT信号注册信号处理函数进行资源清理，A1、B1进程手动关闭即可。 特别注意 A、B通过管道通信，如果首先通过ctrl+c退出A进程，那么B进程的fifo1管道的写端会收到SIGPIPE信号而终止B进程，因此必须在B进程终止前清理掉被B占用的共享内存2，将共享内存2的引用计数减一，否则，当B1进程退出并清理共享内存2后，共享内存2的引用计数不为0，会导致共享内存2得不到释放； 为了解决前一个问题，A、B进程在启动后立即将各自的进程id通过管道发送给对方，并在各自的进程退出时向对方进程id发送SIGINT信号，触发对方进程进入信号处理接口执行资源回收工作； A和A1通过共享内存1通信，会从A进程和A1进程的虚拟地址空间分配一段连续的页映射到同一块连续的物理内存页上，这样A、A1两个进程都可以间接访问物理内存页，从而达到通信的目的，一般共享内存需要进行保护，读写不能同时进行，也不能同时进行写操作，共享内存省去了从内核缓冲区到用户缓冲区的拷贝，因此效率高。 编码与效果图 func.h:12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;strings.h&gt;#include &lt;string.h&gt;#include &lt;sys/select.h&gt;#include &lt;sys/time.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/wait.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;#include &lt;netinet/in.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;string.h&gt;#include &lt;netdb.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/select.h&gt;#include &lt;sys/time.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/uio.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt; processA.cpp：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include \"func.h\"int shmid;int pidB; // 存放对端进程B的进程id号char *p; // 共享内存指针// 回收共享内存资源前先杀死对端进程，否则回收失败void handle(int num)&#123; kill(pidB, SIGINT); shmdt(p); int ret; if(-1 == (ret=shmctl(shmid, IPC_RMID, NULL))) &#123; perror(\"shmctl\"); return (void)-1; &#125; exit(0);&#125;int main(int argc, char **argv)&#123; signal(SIGINT, handle); if(-1 == (shmid=shmget(1234, 4096, IPC_CREAT|0666))) &#123; perror(\"shmget\"); return -1; &#125; if((char*)-1 == (p=(char*)shmat(shmid, NULL, 0))) &#123; perror(\"shmat\"); return -1; &#125; // 管道文件为单工通信方式，因此需要建立两条管道 // A进程通过管道文件fifo1的读端fdr读取B进程发送的数据 // A进程通过管道文件fifo2的写端fdw向B进程发送数据 int fdr, fdw; if(-1 == (fdr=open(\"fifo1\", O_RDONLY)) || -1 == (fdw=open(\"fifo2\", O_WRONLY))) &#123; perror(\"open fifo1 or open fifo2\"); return -1; &#125; // 通信之前先通过管道互相告知对方自己的进程id char s1[10] = &#123;0&#125;; char s2[10] = &#123;0&#125;; sprintf(s1, \"%d\\n\", getpid()); write(fdw, s1, strlen(s1) - 1); read(fdr, s2, strlen(s1) - 1); pidB = atoi(s2); printf(\"pipe connect success, A to A1 shmid:[%d], pidA:[%d], pidB:[%d]\\n\", shmid, getpid(), pidB); char buf[1024] = &#123;0&#125;; int ret; fd_set rdset; while(true) &#123; FD_ZERO(&amp;rdset); FD_SET(0, &amp;rdset); FD_SET(fdr, &amp;rdset); if((ret=select(fdr+1, &amp;rdset, NULL, NULL, NULL) &gt; 0)) &#123; // fdr可读,则接收数据之后通过共享内存传给A1 if(FD_ISSET(fdr, &amp;rdset)) &#123; bzero(buf, sizeof(buf)); if(read(fdr, buf, sizeof(buf)) &gt; 0) &#123; strncpy(p, buf, sizeof(buf)); &#125; else &#123; break; &#125; &#125; // 标准输入可读,读出来传递给B进程 if(FD_ISSET(0, &amp;rdset)) &#123; bzero(buf, sizeof(buf)); if(read(STDIN_FILENO, buf, sizeof(buf)) &gt; 0) &#123; write(fdw, buf, strlen(buf) - 1); &#125; else &#123; break; &#125; &#125; &#125; &#125; close(fdr); close(fdw); return 0;&#125; processB.cpp：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include \"func.h\"int shmid;int pidA; // 存放对端进程idchar *p; // 共享内存指针// 回收共享内存资源前先杀死对端进程，否则回收失败void handle(int num)&#123; kill(pidA, SIGINT); shmdt(p); int ret; if(-1 == (ret=shmctl(shmid, IPC_RMID, NULL))) &#123; perror(\"shmctl\"); return (void)-1; &#125; exit(0);&#125;int main(int argc, char **argv)&#123; signal(SIGINT, handle); if(-1 == (shmid=shmget(1235, 4096, IPC_CREAT|0666))) &#123; perror(\"shmget\"); return -1; &#125; if((char*)-1 == (p=(char*)shmat(shmid, NULL, 0))) &#123; perror(\"shmat\"); return -1; &#125; // 管道文件为单工通信方式 // B进程通过管道文件fifo1的写端fdw向A进程发送数据 // B进程通过管道文件fifo2的读端fdr接收A进程的数据 int fdr, fdw; if(-1 == (fdw=open(\"fifo1\", O_WRONLY)) || -1 == (fdr=open(\"fifo2\", O_RDONLY))) &#123; perror(\"open fifo1 or open fifo2\"); return -1; &#125; // 通信之前先通过管道互相告知对方自己的进程id char s1[10] = &#123;0&#125;; char s2[10] = &#123;0&#125;; sprintf(s1, \"%d\\n\", getpid()); write(fdw, s1, strlen(s1) - 1); read(fdr, s2, strlen(s1) - 1); pidA = atoi(s2); printf(\"pipe connect success, B to B1 shmid:[%d], pidA:[%d], pidB:[%d]\\n\", shmid, pidA, getpid()); char buf[1024] = &#123;0&#125;; int ret; fd_set rdset; while(true) &#123; FD_ZERO(&amp;rdset); FD_SET(0, &amp;rdset); FD_SET(fdr, &amp;rdset); if((ret=select(fdr+1, &amp;rdset, NULL, NULL, NULL) &gt; 0)) &#123; // fdr可读,则接收数据之后通过共享内存传给B1 if(FD_ISSET(fdr, &amp;rdset)) &#123; bzero(buf, sizeof(buf)); if(read(fdr, buf, sizeof(buf)) &gt; 0) &#123; strncpy(p, buf, sizeof(buf)); &#125; else &#123; break; &#125; &#125; // 标注输入可读,读出来传递给A进程 if(FD_ISSET(0, &amp;rdset)) &#123; bzero(buf, sizeof(buf)); if(read(STDIN_FILENO, buf, sizeof(buf)) &gt; 0) &#123; write(fdw, buf, strlen(buf) - 1); &#125; else &#123; break; &#125; &#125; &#125; &#125; close(fdr); close(fdw); return 0;&#125; processA1.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142#include \"func.h\"int main(void)&#123; char buf[1024] = &#123;0&#125;; int shmid; if(-1 == (shmid=shmget(1234, 4096, IPC_CREAT|0666))) &#123; perror(\"shmget\"); return -1; &#125; char *p; if((char*)-1 == (p=(char*)shmat(shmid, NULL, 0))) &#123; perror(\"shmat\"); return -1; &#125; while(true) &#123; if(!(strcmp(buf, p))) &#123; continue; &#125; else &#123; // 共享内存有数据可读 bzero(buf, sizeof(buf)); strcpy(buf, p); printf(\"I am A1, recv from A:[%s]\\n\", buf); &#125; &#125; if(-1 ==(shmctl(shmid, IPC_RMID, 0))) &#123; perror(\"shmctl\"); return -1; &#125; return 0;&#125; processB1.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142#include \"func.h\"int main(void)&#123; char buf[1024] = &#123;0&#125;; int shmid; if(-1 == (shmid=shmget(1235, 4096, IPC_CREAT|0666))) &#123; perror(\"shmget\"); return -1; &#125; char *p; if((char*)-1 == (p=(char*)shmat(shmid, NULL, 0))) &#123; perror(\"shmat\"); return -1; &#125; while(true) &#123; if(!(strcmp(buf, p))) &#123; continue; &#125; else &#123; // 共享内存有数据可读 bzero(buf, sizeof(buf)); strcpy(buf, p); printf(\"I am B1, recv from B:[%s]\\n\", buf); &#125; &#125; if(-1 ==(shmctl(shmid, IPC_RMID, 0))) &#123; perror(\"shmctl\"); return -1; &#125; return 0;&#125; 回收资源 这里首先通过ctrl+c退出A进程，然后B进程收到SIGPIPE信号退出，A、B进程同时调用各自的信号处理函数回收资源，通过ipcs命令发现拥有者为root的共享内存资源的nattch都为1，分别被A1和B1占有。 然后手动关闭A1、B1进程，再次执行ipcs命令，发现拥有者为root的共享内存资源不存在，已经释放成功。1$ ipcs # 查看共性内存资源数量 源码获取本文所有源码链接","categories":[{"name":"IO多路复用模型","slug":"IO多路复用模型","permalink":"https://icoty.github.io/categories/IO多路复用模型/"},{"name":"同步机制","slug":"同步机制","permalink":"https://icoty.github.io/categories/同步机制/"},{"name":"进程间通信","slug":"进程间通信","permalink":"https://icoty.github.io/categories/进程间通信/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://icoty.github.io/tags/Linux/"},{"name":"共享内存","slug":"共享内存","permalink":"https://icoty.github.io/tags/共享内存/"},{"name":"命名管道","slug":"命名管道","permalink":"https://icoty.github.io/tags/命名管道/"},{"name":"信号","slug":"信号","permalink":"https://icoty.github.io/tags/信号/"},{"name":"Select","slug":"Select","permalink":"https://icoty.github.io/tags/Select/"}]},{"title":"Linux下Docker快速部署LAMP","slug":"docker-lamp","date":"2019-04-16T15:23:05.000Z","updated":"2019-05-23T04:05:14.000Z","comments":true,"path":"2019/04/16/docker-lamp/","link":"","permalink":"https://icoty.github.io/2019/04/16/docker-lamp/","excerpt":"","text":"若你的mac或Linux环境上未安装Docker，请移步Docker安装，确认安装成功之后再进行下文内容。如果你不了解Docker如何操作，但是你又想彻底弄懂Docker命令，可以看我另一篇文章Docker最简教程。 拿来即用获取LAMPLAMP镜像我已经搭建好并且我已经测试过了，没有问题。你只需要直接拿去用，执行如下命令：123$docker pull icoty1/lamp:v1.1.0$docker images # 能够看到icoty1/lamp:v1.1.0已经被拉到你本地$docker run -d -ti -p 80:80 -p 3306:3306 -v /Users/yangyu/app/:/var/www/html/ icoty1/lamp:v1.1.0 /bin/bash start.sh # 运行一个容器，目录/Users/yangyu/app/是你本机PHP应用位置 /Users/yangyu/app/下存放的是public、thinkphp、vendor、runtime等内容。然后访问http://localhost 能够看到PHP应用目录下的内容，如下图，说明已经成功。 然后访问http://localhost/public/index.php ，这个是PHP的入口。如果浏览器打开提示权限不够，不要慌，检查下你无法访问的那个目录下是否存在.htaccess文件，如果有则删除就好了，如果没有则执行如下命令。 123$docker exec -it [CONTAINER ID] /bin/bash # 进入前面启动的容器中$chmod -R 0777 /var/www/html/ # 赋予最高权限$sh start.sh # start.sh在根目录下，是重启服务用的 访问phpadmin：http://localhost/phpmyadmin/index.php ，登陆的用户名和密码均为phpmyadmin，登陆后你能够在浏览器上一目了然的对所有数据表进行操作。 容器内根目录下有个start.sh文件，每次需要重启apache服务和mysql服务时只需要执行这个脚本就好了，命令如下： 1$sh start.sh LAMP版本Ubuntu 18.04.2，PHP 7.2.15，mysql 5.7.25，同时也安装了phpmyadmin。下面是查看版本的命令。mysql数据库的root账户密码是root，phpmyadmin账户密码是phpmyadmin ，你可以把密码修改成你的，mysql修改用户密码。 12345678910111213root@4f5e11ebccac:/# cat /etc/issueUbuntu 18.04.2 LTS \\n \\lroot@4f5e11ebccac:/# php -vPHP 7.2.15-0ubuntu0.18.04.2 (cli) (built: Mar 22 2019 17:05:14) ( NTS )Copyright (c) 1997-2018 The PHP GroupZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies with Zend OPcache v7.2.15-0ubuntu0.18.04.2, Copyright (c) 1999-2018, by Zend Technologiesroot@4f5e11ebccac:/# mysql -u root -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 9Server version: 5.7.25-0ubuntu0.18.04.2 (Ubuntu) 到这里你的目的就已经达到了，一个完整LAMP服务已经在你本机上跑起来并且能用了。下面的内容是我制作icoty1/lamp:v1.1.0的过程，如果你有兴趣，或者想知道我是怎么制作出来的，欢迎继续围观。 icoty1/lamp:v1.1.0制作过程获取ubuntu基础镜像1$ docker pull i icoty1/ubuntu:18.04.2-LTS # 从icoty1仓库拉取基础镜像并运行一个容器 安装依赖进入前面运行的容器中安装接下来的内容。 mysql12345$apt-get update$apt-get upgrade -y $apt-get dist-upgrade -y$apt-get install vim -y$apt-get install mysql-server mysql-client -y apache/php12345$apt-get install apache2 -y$vi /etc/apache2/apache2.conf # 添加 ServerName localhost:80$apt-get install php7.2 -y # 这个过程中需要选择国家和时区，如图。$apt-get install libapache2-mod-php7.2$apt-get install php7.2-mysql -y phpmyadmin123$apt-get install php-mbstring php7.0-mbstring php-gettext$service apache2 restart$apt-get install phpmyadmin # 这个过程中会自动创建mysql用户名phpmyadmin，需要手动输入密码，如图。 使apache解析php文件 vi /etc/apache2/apache2.conf，添加如下内容，让apache服务知道libphp7.2.so库在哪里，找不到这个动态库就无法解析php文件。1234# add by yangyu, current dictory is '/etc/apache2/', so '../../usr/lib/apache2/modules/libphp7.2.so' = '/usr/lib/apache2/modules/libphp7.2.so'LoadModule php7_module ../../usr/lib/apache2/modules/libphp7.2.soAddType application/x-httpd-php .phpDirectoryIndex index.php index.htm index.html 到此，这个容器内已经搭建好了LAMP服务，使用docker commit命令把这个容器提交为镜像icoty1/lamp:v1.1.0，然后push到我的docker hub仓库上，你所pull的正是我push上去的。 参考文献https://www.cnblogs.com/impy/p/8040684.html # lamphttps://linux.cn/article-7463-1.html # lamphttps://blog.csdn.net/longgeaisisi/article/details/78448525 # lamphttps://www.cnblogs.com/mmx8861/p/9062363.html # mysql密码修改","categories":[{"name":"Docker","slug":"Docker","permalink":"https://icoty.github.io/categories/Docker/"},{"name":"LAMP","slug":"LAMP","permalink":"https://icoty.github.io/categories/LAMP/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://icoty.github.io/tags/Linux/"},{"name":"LAMP","slug":"LAMP","permalink":"https://icoty.github.io/tags/LAMP/"},{"name":"Dockerfile","slug":"Dockerfile","permalink":"https://icoty.github.io/tags/Dockerfile/"}]}]}